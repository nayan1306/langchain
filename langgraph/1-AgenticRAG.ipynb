{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19ab913a",
   "metadata": {},
   "source": [
    "### Agentic RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da2985b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64c96f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "706e2335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/tutorials/introduction/', 'title': 'Redirecting...', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nRedirecting...\\n\\n\\n\\n\\n\\n\\nRedirecting...\\n\\n\\n')],\n",
       " [Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/tutorials/workflows/', 'title': 'Redirecting...', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nRedirecting...\\n\\n\\n\\n\\n\\n\\nRedirecting...\\n\\n\\n')],\n",
       " [Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/how-tos/map-reduce/', 'title': 'Redirecting...', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nRedirecting...\\n\\n\\n\\n\\n\\n\\nRedirecting...\\n\\n\\n')]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls=[\n",
    "    \"https://langchain-ai.github.io/langgraph/tutorials/introduction/\",\n",
    "    \"https://langchain-ai.github.io/langgraph/tutorials/workflows/\",\n",
    "    \"https://langchain-ai.github.io/langgraph/how-tos/map-reduce/\"\n",
    "]\n",
    "\n",
    "docs=[WebBaseLoader(url).load() for url in urls]\n",
    "docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a08aee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=100\n",
    ")\n",
    "\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "## Add alll these text to vectordb\n",
    "\n",
    "vectorstore=FAISS.from_documents(\n",
    "    documents=doc_splits,\n",
    "    embedding=OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "\n",
    "retriever=vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ea4cf11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='91e89d8b-8303-4736-abf0-b05ee4b868ce', metadata={'source': 'https://langchain-ai.github.io/langgraph/tutorials/introduction/', 'title': 'Redirecting...', 'language': 'en'}, page_content='Redirecting...\\n\\n\\n\\n\\n\\n\\nRedirecting...'),\n",
       " Document(id='d96a7b0f-f5ca-4ee5-807e-2e694d55497c', metadata={'source': 'https://langchain-ai.github.io/langgraph/tutorials/workflows/', 'title': 'Redirecting...', 'language': 'en'}, page_content='Redirecting...\\n\\n\\n\\n\\n\\n\\nRedirecting...'),\n",
       " Document(id='0106f423-e7da-4383-9dad-e78fa9ec4096', metadata={'source': 'https://langchain-ai.github.io/langgraph/how-tos/map-reduce/', 'title': 'Redirecting...', 'language': 'en'}, page_content='Redirecting...\\n\\n\\n\\n\\n\\n\\nRedirecting...')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"what is langgraph\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0a7a99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import create_retriever_tool\n",
    "\n",
    "\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever=retriever,\n",
    "    name=\"retriever_vector_db\",\n",
    "    description=\"Search for information about LangGraph from the vector database.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "abcc6a53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructuredTool(name='retriever_vector_db', description='Search for information about LangGraph from the vector database.', args_schema=<class 'langchain_core.tools.retriever.RetrieverInput'>, func=<function create_retriever_tool.<locals>.func at 0x113689640>, coroutine=<function create_retriever_tool.<locals>.afunc at 0x1136894e0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ac6e08",
   "metadata": {},
   "source": [
    "### Langchain Blogs- Seperate Vector Store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c2ed2c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/', 'title': 'Build a RAG agent with LangChain - Docs by LangChain', 'language': 'en'}, page_content='Build a RAG agent with LangChain - Docs by LangChainSkip to main contentDocs by LangChain home pageLangChain + LangGraphSearch...âŒ˜KAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationLangChainBuild a RAG agent with LangChainLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonLearnTutorialsLangChainSemantic searchRAG agentSQL agentVoice agentMulti-agentLangGraphConceptual overviewsComponent architectureMemoryContextGraph APIFunctional APIAdditional resourcesLangChain AcademyCase studiesGet helpOn this pageOverviewConceptsPreviewSetupInstallationLangSmithComponents1. IndexingLoading documentsSplitting documentsStoring documents2. Retrieval and GenerationRAG agentsRAG chainsNext stepsTutorialsLangChainBuild a RAG agent with LangChainCopy pageCopy page\\u200bOverview\\nOne of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.\\nThis tutorial will show how to build a simple Q&A application over an unstructured text data source. We will demonstrate:\\n\\nA RAG agent that executes searches with a simple tool. This is a good general-purpose implementation.\\nA two-step RAG chain that uses just a single LLM call per query. This is a fast and effective method for simple queries.\\n\\n\\u200bConcepts\\nWe will cover the following concepts:\\n\\n\\nIndexing: a pipeline for ingesting data from a source and indexing it. This usually happens in a separate process.\\n\\n\\nRetrieval and generation: the actual RAG process, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\\n\\n\\nOnce weâ€™ve indexed our data, we will use an agent as our orchestration framework to implement the retrieval and generation steps.\\nThe indexing portion of this tutorial will largely follow the semantic search tutorial.If your data is already available for search (i.e., you have a function to execute a search), or youâ€™re comfortable with the content from that tutorial, feel free to skip to the section on retrieval and generation\\n\\u200bPreview\\nIn this guide weâ€™ll build an app that answers questions about the websiteâ€™s content. The specific website we will use is the LLM Powered Autonomous Agents blog post by Lilian Weng, which allows us to ask questions about the contents of the post.\\nWe can create a simple indexing pipeline and RAG chain to do this in ~40 lines of code. See below for the full code snippet:\\nExpand for full code snippetCopyimport bs4\\nfrom langchain.agents import AgentState, create_agent\\nfrom langchain_community.document_loaders import WebBaseLoader\\nfrom langchain.messages import MessageLikeRepresentation\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\n\\n# Load and chunk contents of the blog\\nloader = WebBaseLoader(\\n    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\\n    bs_kwargs=dict(\\n        parse_only=bs4.SoupStrainer(\\n            class_=(\"post-content\", \"post-title\", \"post-header\")\\n        )\\n    ),\\n)\\ndocs = loader.load()\\n\\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\\nall_splits = text_splitter.split_documents(docs)\\n\\n# Index chunks\\n_ = vector_store.add_documents(documents=all_splits)\\n\\n# Construct a tool for retrieving context\\n@tool(response_format=\"content_and_artifact\")\\ndef retrieve_context(query: str):\\n    \"\"\"Retrieve information to help answer a query.\"\"\"\\n    retrieved_docs = vector_store.similarity_search(query, k=2)\\n    serialized = \"\\\\n\\\\n\".join(\\n        (f\"Source: {doc.metadata}\\\\nContent: {doc.page_content}\")\\n        for doc in retrieved_docs\\n    )\\n    return serialized, retrieved_docs\\n\\ntools = [retrieve_context]\\n# If desired, specify custom instructions\\nprompt = (\\n    \"You have access to a tool that retrieves context from a blog post. \"\\n    \"Use the tool to help answer user queries.\"\\n)\\nagent = create_agent(model, tools, system_prompt=prompt)\\nCopyquery = \"What is task decomposition?\"\\nfor step in agent.stream(\\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\\n    stream_mode=\"values\",\\n):\\n    step[\"messages\"][-1].pretty_print()\\nCopy================================ Human Message =================================\\n\\nWhat is task decomposition?\\n================================== Ai Message ==================================\\nTool Calls:\\n  retrieve_context (call_xTkJr8njRY0geNz43ZvGkX0R)\\n Call ID: call_xTkJr8njRY0geNz43ZvGkX0R\\n  Args:\\n    query: task decomposition\\n================================= Tool Message =================================\\nName: retrieve_context\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Task decomposition can be done by...\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Component One: Planning...\\n================================== Ai Message ==================================\\n\\nTask decomposition refers to...\\nCheck out the LangSmith trace.\\n\\u200bSetup\\n\\u200bInstallation\\nThis tutorial requires these langchain dependencies:\\npipuvCopypip install langchain langchain-text-splitters langchain-community bs4\\n\\nFor more details, see our Installation guide.\\n\\u200bLangSmith\\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith.\\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\\nCopyexport LANGSMITH_TRACING=\"true\"\\nexport LANGSMITH_API_KEY=\"...\"\\n\\nOr, set them in Python:\\nCopyimport getpass\\nimport os\\n\\nos.environ[\"LANGSMITH_TRACING\"] = \"true\"\\nos.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\\n\\n\\u200bComponents\\nWe will need to select three components from LangChainâ€™s suite of integrations.\\nSelect a chat model:\\n OpenAI Anthropic Azure Google Gemini AWS Bedrock HuggingFaceðŸ‘‰ Read the OpenAI chat model integration docsCopypip install -U \"langchain[openai]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\\n\\nmodel = init_chat_model(\"gpt-4.1\")\\nðŸ‘‰ Read the Anthropic chat model integration docsCopypip install -U \"langchain[anthropic]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"ANTHROPIC_API_KEY\"] = \"sk-...\"\\n\\nmodel = init_chat_model(\"claude-sonnet-4-5-20250929\")\\nðŸ‘‰ Read the Azure chat model integration docsCopypip install -U \"langchain[openai]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"AZURE_OPENAI_API_KEY\"] = \"...\"\\nos.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"...\"\\nos.environ[\"OPENAI_API_VERSION\"] = \"2025-03-01-preview\"\\n\\nmodel = init_chat_model(\\n    \"azure_openai:gpt-4.1\",\\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\\n)\\nðŸ‘‰ Read the Google GenAI chat model integration docsCopypip install -U \"langchain[google-genai]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"GOOGLE_API_KEY\"] = \"...\"\\n\\nmodel = init_chat_model(\"google_genai:gemini-2.5-flash-lite\")\\nðŸ‘‰ Read the AWS Bedrock chat model integration docsCopypip install -U \"langchain[aws]\"\\ninit_chat_modelModel ClassCopyfrom langchain.chat_models import init_chat_model\\n\\n# Follow the steps here to configure your credentials:\\n# https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html\\n\\nmodel = init_chat_model(\\n    \"anthropic.claude-3-5-sonnet-20240620-v1:0\",\\n    model_provider=\"bedrock_converse\",\\n)\\nðŸ‘‰ Read the HuggingFace chat model integration docsCopypip install -U \"langchain[huggingface]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"hf_...\"\\n\\nmodel = init_chat_model(\\n    \"microsoft/Phi-3-mini-4k-instruct\",\\n    model_provider=\"huggingface\",\\n    temperature=0.7,\\n    max_tokens=1024,\\n)\\n\\nSelect an embeddings model:\\n OpenAI Azure Google Gemini Google Vertex AWS HuggingFace Ollama Cohere MistralAI Nomic NVIDIA Voyage AI IBM watsonx Fake IsaacusCopypip install -U \"langchain-openai\"\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"OPENAI_API_KEY\"):\\n    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\\n\\nfrom langchain_openai import OpenAIEmbeddings\\n\\nembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\\nCopypip install -U \"langchain-openai\"\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"AZURE_OPENAI_API_KEY\"):\\n    os.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for Azure: \")\\n\\nfrom langchain_openai import AzureOpenAIEmbeddings\\n\\nembeddings = AzureOpenAIEmbeddings(\\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\\n)\\nCopypip install -qU langchain-google-genai\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"GOOGLE_API_KEY\"):\\n    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")\\n\\nfrom langchain_google_genai import GoogleGenerativeAIEmbeddings\\n\\nembeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")\\nCopypip install -qU langchain-google-vertexai\\nCopyfrom langchain_google_vertexai import VertexAIEmbeddings\\n\\nembeddings = VertexAIEmbeddings(model=\"text-embedding-005\")\\nCopypip install -qU langchain-aws\\nCopyfrom langchain_aws import BedrockEmbeddings\\n\\nembeddings = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v2:0\")\\nCopypip install -qU langchain-huggingface\\nCopyfrom langchain_huggingface import HuggingFaceEmbeddings\\n\\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\\nCopypip install -qU langchain-ollama\\nCopyfrom langchain_ollama import OllamaEmbeddings\\n\\nembeddings = OllamaEmbeddings(model=\"llama3\")\\nCopypip install -qU langchain-cohere\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"COHERE_API_KEY\"):\\n    os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Enter API key for Cohere: \")\\n\\nfrom langchain_cohere import CohereEmbeddings\\n\\nembeddings = CohereEmbeddings(model=\"embed-english-v3.0\")\\nCopypip install -qU langchain-mistralai\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"MISTRALAI_API_KEY\"):\\n    os.environ[\"MISTRALAI_API_KEY\"] = getpass.getpass(\"Enter API key for MistralAI: \")\\n\\nfrom langchain_mistralai import MistralAIEmbeddings\\n\\nembeddings = MistralAIEmbeddings(model=\"mistral-embed\")\\nCopypip install -qU langchain-nomic\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"NOMIC_API_KEY\"):\\n    os.environ[\"NOMIC_API_KEY\"] = getpass.getpass(\"Enter API key for Nomic: \")\\n\\nfrom langchain_nomic import NomicEmbeddings\\n\\nembeddings = NomicEmbeddings(model=\"nomic-embed-text-v1.5\")\\nCopypip install -qU langchain-nvidia-ai-endpoints\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"NVIDIA_API_KEY\"):\\n    os.environ[\"NVIDIA_API_KEY\"] = getpass.getpass(\"Enter API key for NVIDIA: \")\\n\\nfrom langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\\n\\nembeddings = NVIDIAEmbeddings(model=\"NV-Embed-QA\")\\nCopypip install -qU langchain-voyageai\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"VOYAGE_API_KEY\"):\\n    os.environ[\"VOYAGE_API_KEY\"] = getpass.getpass(\"Enter API key for Voyage AI: \")\\n\\nfrom langchain-voyageai import VoyageAIEmbeddings\\n\\nembeddings = VoyageAIEmbeddings(model=\"voyage-3\")\\nCopypip install -qU langchain-ibm\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"WATSONX_APIKEY\"):\\n    os.environ[\"WATSONX_APIKEY\"] = getpass.getpass(\"Enter API key for IBM watsonx: \")\\n\\nfrom langchain_ibm import WatsonxEmbeddings\\n\\nembeddings = WatsonxEmbeddings(\\n    model_id=\"ibm/slate-125m-english-rtrvr\",\\n    url=\"https://us-south.ml.cloud.ibm.com\",\\n    project_id=\"<WATSONX PROJECT_ID>\",\\n)\\nCopypip install -qU langchain-core\\nCopyfrom langchain_core.embeddings import DeterministicFakeEmbedding\\n\\nembeddings = DeterministicFakeEmbedding(size=4096)\\nCopypip install -qU langchain-isaacus\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"ISAACUS_API_KEY\"):\\nos.environ[\"ISAACUS_API_KEY\"] = getpass.getpass(\"Enter API key for Isaacus: \")\\n\\nfrom langchain_isaacus import IsaacusEmbeddings\\n\\nembeddings = IsaacusEmbeddings(model=\"kanon-2-embedder\")\\n\\nSelect a vector store:\\n In-memory Amazon OpenSearch AstraDB Chroma FAISS Milvus MongoDB PGVector PGVectorStore Pinecone QdrantCopypip install -U \"langchain-core\"\\nCopyfrom langchain_core.vectorstores import InMemoryVectorStore\\n\\nvector_store = InMemoryVectorStore(embeddings)\\nCopypip install -qU  boto3\\nCopyfrom opensearchpy import RequestsHttpConnection\\n\\nservice = \"es\"  # must set the service as \\'es\\'\\nregion = \"us-east-2\"\\ncredentials = boto3.Session(\\n    aws_access_key_id=\"xxxxxx\", aws_secret_access_key=\"xxxxx\"\\n).get_credentials()\\nawsauth = AWS4Auth(\"xxxxx\", \"xxxxxx\", region, service, session_token=credentials.token)\\n\\nvector_store = OpenSearchVectorSearch.from_documents(\\n    docs,\\n    embeddings,\\n    opensearch_url=\"host url\",\\n    http_auth=awsauth,\\n    timeout=300,\\n    use_ssl=True,\\n    verify_certs=True,\\n    connection_class=RequestsHttpConnection,\\n    index_name=\"test-index\",\\n)\\nCopypip install -U \"langchain-astradb\"\\nCopyfrom langchain_astradb import AstraDBVectorStore\\n\\nvector_store = AstraDBVectorStore(\\n    embedding=embeddings,\\n    api_endpoint=ASTRA_DB_API_ENDPOINT,\\n    collection_name=\"astra_vector_langchain\",\\n    token=ASTRA_DB_APPLICATION_TOKEN,\\n    namespace=ASTRA_DB_NAMESPACE,\\n)\\nCopypip install -qU langchain-chroma\\nCopyfrom langchain_chroma import Chroma\\n\\nvector_store = Chroma(\\n    collection_name=\"example_collection\",\\n    embedding_function=embeddings,\\n    persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not necessary\\n)\\nCopypip install -qU langchain-community faiss-cpu\\nCopyimport faiss\\nfrom langchain_community.docstore.in_memory import InMemoryDocstore\\nfrom langchain_community.vectorstores import FAISS\\n\\nembedding_dim = len(embeddings.embed_query(\"hello world\"))\\nindex = faiss.IndexFlatL2(embedding_dim)\\n\\nvector_store = FAISS(\\n    embedding_function=embeddings,\\n    index=index,\\n    docstore=InMemoryDocstore(),\\n    index_to_docstore_id={},\\n)\\nCopypip install -qU langchain-milvus\\nCopyfrom langchain_milvus import Milvus\\n\\nURI = \"./milvus_example.db\"\\n\\nvector_store = Milvus(\\n    embedding_function=embeddings,\\n    connection_args={\"uri\": URI},\\n    index_params={\"index_type\": \"FLAT\", \"metric_type\": \"L2\"},\\n)\\nCopypip install -qU langchain-mongodb\\nCopyfrom langchain_mongodb import MongoDBAtlasVectorSearch\\n\\nvector_store = MongoDBAtlasVectorSearch(\\n    embedding=embeddings,\\n    collection=MONGODB_COLLECTION,\\n    index_name=ATLAS_VECTOR_SEARCH_INDEX_NAME,\\n    relevance_score_fn=\"cosine\",\\n)\\nCopypip install -qU langchain-postgres\\nCopyfrom langchain_postgres import PGVector\\n\\nvector_store = PGVector(\\n    embeddings=embeddings,\\n    collection_name=\"my_docs\",\\n    connection=\"postgresql+psycopg://...\",\\n)\\nCopypip install -qU langchain-postgres\\nCopyfrom langchain_postgres import PGEngine, PGVectorStore\\n\\npg_engine = PGEngine.from_connection_string(\\n    url=\"postgresql+psycopg://...\"\\n)\\n\\nvector_store = PGVectorStore.create_sync(\\n    engine=pg_engine,\\n    table_name=\\'test_table\\',\\n    embedding_service=embedding\\n)\\nCopypip install -qU langchain-pinecone\\nCopyfrom langchain_pinecone import PineconeVectorStore\\nfrom pinecone import Pinecone\\n\\npc = Pinecone(api_key=...)\\nindex = pc.Index(index_name)\\n\\nvector_store = PineconeVectorStore(embedding=embeddings, index=index)\\nCopypip install -qU langchain-qdrant\\nCopyfrom qdrant_client.models import Distance, VectorParams\\nfrom langchain_qdrant import QdrantVectorStore\\nfrom qdrant_client import QdrantClient\\n\\nclient = QdrantClient(\":memory:\")\\n\\nvector_size = len(embeddings.embed_query(\"sample text\"))\\n\\nif not client.collection_exists(\"test\"):\\n    client.create_collection(\\n        collection_name=\"test\",\\n        vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE)\\n    )\\nvector_store = QdrantVectorStore(\\n    client=client,\\n    collection_name=\"test\",\\n    embedding=embeddings,\\n)\\n\\n\\u200b1. Indexing\\nThis section is an abbreviated version of the content in the semantic search tutorial.If your data is already indexed and available for search (i.e., you have a function to execute a search), or if youâ€™re comfortable with document loaders, embeddings, and vector stores, feel free to skip to the next section on retrieval and generation.\\nIndexing commonly works as follows:\\n\\nLoad: First we need to load our data. This is done with Document Loaders.\\nSplit: Text splitters break large Documents into smaller chunks. This is useful both for indexing data and passing it into a model, as large chunks are harder to search over and wonâ€™t fit in a modelâ€™s finite context window.\\nStore: We need somewhere to store and index our splits, so that they can be searched over later. This is often done using a VectorStore and Embeddings model.\\n\\n\\n\\u200bLoading documents\\nWe need to first load the blog post contents. We can use DocumentLoaders for this, which are objects that load in data from a source and return a list of Document objects.\\nIn this case weâ€™ll use the WebBaseLoader, which uses urllib to load HTML from web URLs and BeautifulSoup to parse it to text. We can customize the HTML -> text parsing by passing in parameters into the BeautifulSoup parser via bs_kwargs (see BeautifulSoup docs). In this case only HTML tags with class â€œpost-contentâ€, â€œpost-titleâ€, or â€œpost-headerâ€ are relevant, so weâ€™ll remove all others.\\nCopyimport bs4\\nfrom langchain_community.document_loaders import WebBaseLoader\\n\\n# Only keep post title, headers, and content from the full HTML.\\nbs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\\nloader = WebBaseLoader(\\n    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\\n    bs_kwargs={\"parse_only\": bs4_strainer},\\n)\\ndocs = loader.load()\\n\\nassert len(docs) == 1\\nprint(f\"Total characters: {len(docs[0].page_content)}\")\\n\\nCopyTotal characters: 43131\\n\\nCopyprint(docs[0].page_content[:500])\\n\\nCopy      LLM Powered Autonomous Agents\\n\\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn\\n\\nGo deeper\\nDocumentLoader: Object that loads data from a source as list of Documents.\\n\\nIntegrations: 160+ integrations to choose from.\\nBaseLoader: API reference for the base interface.\\n\\n\\u200bSplitting documents\\nOur loaded document is over 42k characters which is too long to fit into the context window of many models. Even for those models that could fit the full post in their context window, models can struggle to find information in very long inputs.\\nTo handle this weâ€™ll split the Document into chunks for embedding and vector storage. This should help us retrieve only the most relevant parts of the blog post at run time.\\nAs in the semantic search tutorial, we use a RecursiveCharacterTextSplitter, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases.\\nCopyfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\n\\ntext_splitter = RecursiveCharacterTextSplitter(\\n    chunk_size=1000,  # chunk size (characters)\\n    chunk_overlap=200,  # chunk overlap (characters)\\n    add_start_index=True,  # track index in original document\\n)\\nall_splits = text_splitter.split_documents(docs)\\n\\nprint(f\"Split blog post into {len(all_splits)} sub-documents.\")\\n\\nCopySplit blog post into 66 sub-documents.\\n\\nGo deeper\\nTextSplitter: Object that splits a list of Document objects into smaller\\nchunks for storage and retrieval.\\n\\nIntegrations\\nInterface: API reference for the base interface.\\n\\n\\u200bStoring documents\\nNow we need to index our 66 text chunks so that we can search over them at runtime. Following the semantic search tutorial, our approach is to embed the contents of each document split and insert these embeddings into a vector store. Given an input query, we can then use vector search to retrieve relevant documents.\\nWe can embed and store all of our document splits in a single command using the vector store and embeddings model selected at the start of the tutorial.\\nCopydocument_ids = vector_store.add_documents(documents=all_splits)\\n\\nprint(document_ids[:3])\\n\\nCopy[\\'07c18af6-ad58-479a-bfb1-d508033f9c64\\', \\'9000bf8e-1993-446f-8d4d-f4e507ba4b8f\\', \\'ba3b5d14-bed9-4f5f-88be-44c88aedc2e6\\']\\n\\nGo deeper\\nEmbeddings: Wrapper around a text embedding model, used for converting text to embeddings.\\n\\nIntegrations: 30+ integrations to choose from.\\nInterface: API reference for the base interface.\\n\\nVectorStore: Wrapper around a vector database, used for storing and querying embeddings.\\n\\nIntegrations: 40+ integrations to choose from.\\nInterface: API reference for the base interface.\\n\\nThis completes the Indexing portion of the pipeline. At this point we have a query-able vector store containing the chunked contents of our blog post. Given a user question, we should ideally be able to return the snippets of the blog post that answer the question.\\n\\u200b2. Retrieval and Generation\\nRAG applications commonly work as follows:\\n\\nRetrieve: Given a user input, relevant splits are retrieved from storage using a Retriever.\\nGenerate: A model produces an answer using a prompt that includes both the question with the retrieved data\\n\\n\\nNow letâ€™s write the actual application logic. We want to create a simple application that takes a user question, searches for documents relevant to that question, passes the retrieved documents and initial question to a model, and returns an answer.\\nWe will demonstrate:\\n\\nA RAG agent that executes searches with a simple tool. This is a good general-purpose implementation.\\nA two-step RAG chain that uses just a single LLM call per query. This is a fast and effective method for simple queries.\\n\\n\\u200bRAG agents\\nOne formulation of a RAG application is as a simple agent with a tool that retrieves information. We can assemble a minimal RAG agent by implementing a tool that wraps our vector store:\\nCopyfrom langchain.tools import tool\\n\\n@tool(response_format=\"content_and_artifact\")\\ndef retrieve_context(query: str):\\n    \"\"\"Retrieve information to help answer a query.\"\"\"\\n    retrieved_docs = vector_store.similarity_search(query, k=2)\\n    serialized = \"\\\\n\\\\n\".join(\\n        (f\"Source: {doc.metadata}\\\\nContent: {doc.page_content}\")\\n        for doc in retrieved_docs\\n    )\\n    return serialized, retrieved_docs\\n\\nHere we use the tool decorator to configure the tool to attach raw documents as artifacts to each ToolMessage. This will let us access document metadata in our application, separate from the stringified representation that is sent to the model.\\nRetrieval tools are not limited to a single string query argument, as in the above example. You can\\nforce the LLM to specify additional search parameters by adding argumentsâ€” for example, a category:Copyfrom typing import Literal\\n\\ndef retrieve_context(query: str, section: Literal[\"beginning\", \"middle\", \"end\"]):\\n\\nGiven our tool, we can construct the agent:\\nCopyfrom langchain.agents import create_agent\\n\\n\\ntools = [retrieve_context]\\n# If desired, specify custom instructions\\nprompt = (\\n    \"You have access to a tool that retrieves context from a blog post. \"\\n    \"Use the tool to help answer user queries.\"\\n)\\nagent = create_agent(model, tools, system_prompt=prompt)\\n\\nLetâ€™s test this out. We construct a question that would typically require an iterative sequence of retrieval steps to answer:\\nCopyquery = (\\n    \"What is the standard method for Task Decomposition?\\\\n\\\\n\"\\n    \"Once you get the answer, look up common extensions of that method.\"\\n)\\n\\nfor event in agent.stream(\\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\\n    stream_mode=\"values\",\\n):\\n    event[\"messages\"][-1].pretty_print()\\n\\nCopy================================ Human Message =================================\\n\\nWhat is the standard method for Task Decomposition?\\n\\nOnce you get the answer, look up common extensions of that method.\\n================================== Ai Message ==================================\\nTool Calls:\\n  retrieve_context (call_d6AVxICMPQYwAKj9lgH4E337)\\n Call ID: call_d6AVxICMPQYwAKj9lgH4E337\\n  Args:\\n    query: standard method for Task Decomposition\\n================================= Tool Message =================================\\nName: retrieve_context\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Task decomposition can be done...\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Component One: Planning...\\n================================== Ai Message ==================================\\nTool Calls:\\n  retrieve_context (call_0dbMOw7266jvETbXWn4JqWpR)\\n Call ID: call_0dbMOw7266jvETbXWn4JqWpR\\n  Args:\\n    query: common extensions of the standard method for Task Decomposition\\n================================= Tool Message =================================\\nName: retrieve_context\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Task decomposition can be done...\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Component One: Planning...\\n================================== Ai Message ==================================\\n\\nThe standard method for Task Decomposition often used is the Chain of Thought (CoT)...\\n\\nNote that the agent:\\n\\nGenerates a query to search for a standard method for task decomposition;\\nReceiving the answer, generates a second query to search for common extensions of it;\\nHaving received all necessary context, answers the question.\\n\\nWe can see the full sequence of steps, along with latency and other metadata, in the LangSmith trace.\\nYou can add a deeper level of control and customization using the LangGraph framework directlyâ€” for example, you can add steps to grade document relevance and rewrite search queries. Check out LangGraphâ€™s Agentic RAG tutorial for more advanced formulations.\\n\\u200bRAG chains\\nIn the above agentic RAG formulation we allow the LLM to use its discretion in generating a tool call to help answer user queries. This is a good general-purpose solution, but comes with some trade-offs:\\nâœ… Benefitsâš ï¸ DrawbacksSearch only when needed â€“ The LLM can handle greetings, follow-ups, and simple queries without triggering unnecessary searches.Two inference calls â€“ When a search is performed, it requires one call to generate the query and another to produce the final response.Contextual search queries â€“ By treating search as a tool with a query input, the LLM crafts its own queries that incorporate conversational context.Reduced control â€“ The LLM may skip searches when they are actually needed, or issue extra searches when unnecessary.Multiple searches allowed â€“ The LLM can execute several searches in support of a single user query.\\nAnother common approach is a two-step chain, in which we always run a search (potentially using the raw user query) and incorporate the result as context for a single LLM query. This results in a single inference call per query, buying reduced latency at the expense of flexibility.\\nIn this approach we no longer call the model in a loop, but instead make a single pass.\\nWe can implement this chain by removing tools from the agent and instead incorporating the retrieval step into a custom prompt:\\nCopyfrom langchain.agents.middleware import dynamic_prompt, ModelRequest\\n\\n@dynamic_prompt\\ndef prompt_with_context(request: ModelRequest) -> str:\\n    \"\"\"Inject context into state messages.\"\"\"\\n    last_query = request.state[\"messages\"][-1].text\\n    retrieved_docs = vector_store.similarity_search(last_query)\\n\\n    docs_content = \"\\\\n\\\\n\".join(doc.page_content for doc in retrieved_docs)\\n\\n    system_message = (\\n        \"You are a helpful assistant. Use the following context in your response:\"\\n        f\"\\\\n\\\\n{docs_content}\"\\n    )\\n\\n    return system_message\\n\\n\\nagent = create_agent(model, tools=[], middleware=[prompt_with_context])\\n\\nLetâ€™s try this out:\\nCopyquery = \"What is task decomposition?\"\\nfor step in agent.stream(\\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\\n    stream_mode=\"values\",\\n):\\n    step[\"messages\"][-1].pretty_print()\\n\\nCopy================================ Human Message =================================\\n\\nWhat is task decomposition?\\n================================== Ai Message ==================================\\n\\nTask decomposition is...\\n\\nIn the LangSmith trace we can see the retrieved context incorporated into the model prompt.\\nThis is a fast and effective method for simple queries in constrained settings, when we typically do want to run user queries through semantic search to pull additional context.\\nReturning source documentsThe above RAG chain incorporates retrieved context into a single system message for that run.As in the agentic RAG formulation, we sometimes want to include raw source documents in the application state to have access to document metadata. We can do this for the two-step chain case by:\\nAdding a key to the state to store the retrieved documents\\nAdding a new node via a pre-model hook to populate that key (as well as inject the context).\\nCopyfrom typing import Any\\nfrom langchain_core.documents import Document\\nfrom langchain.agents.middleware import AgentMiddleware, AgentState\\n\\n\\nclass State(AgentState):\\n    context: list[Document]\\n\\n\\nclass RetrieveDocumentsMiddleware(AgentMiddleware[State]):\\n    state_schema = State\\n\\n    def before_model(self, state: AgentState) -> dict[str, Any] | None:\\n        last_message = state[\"messages\"][-1]\\n        retrieved_docs = vector_store.similarity_search(last_message.text)\\n\\n        docs_content = \"\\\\n\\\\n\".join(doc.page_content for doc in retrieved_docs)\\n\\n        augmented_message_content = (\\n            f\"{last_message.text}\\\\n\\\\n\"\\n            \"Use the following context to answer the query:\\\\n\"\\n            f\"{docs_content}\"\\n        )\\n        return {\\n            \"messages\": [last_message.model_copy(update={\"content\": augmented_message_content})],\\n            \"context\": retrieved_docs,\\n        }\\n\\n\\nagent = create_agent(\\n    model,\\n    tools=[],\\n    middleware=[RetrieveDocumentsMiddleware()],\\n)\\n\\n\\u200bNext steps\\nNow that weâ€™ve implemented a simple RAG application via create_agent, we can easily incorporate new features and go deeper:\\n\\nStream tokens and other information for responsive user experiences\\nAdd conversational memory to support multi-turn interactions\\nAdd long-term memory to support memory across conversational threads\\nAdd structured responses\\nDeploy your application with LangSmith Deployment\\n\\n\\nEdit this page on GitHub or file an issue.\\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.Was this page helpful?YesNoBuild a semantic search engine with LangChainPreviousBuild a SQL agentNextâŒ˜IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by')],\n",
       " [Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/chatbot/', 'title': 'Build a RAG agent with LangChain - Docs by LangChain', 'language': 'en'}, page_content='Build a RAG agent with LangChain - Docs by LangChainSkip to main contentDocs by LangChain home pageLangChain + LangGraphSearch...âŒ˜KAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationLangChainBuild a RAG agent with LangChainLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonLearnTutorialsLangChainSemantic searchRAG agentSQL agentVoice agentMulti-agentLangGraphConceptual overviewsComponent architectureMemoryContextGraph APIFunctional APIAdditional resourcesLangChain AcademyCase studiesGet helpOn this pageOverviewConceptsPreviewSetupInstallationLangSmithComponents1. IndexingLoading documentsSplitting documentsStoring documents2. Retrieval and GenerationRAG agentsRAG chainsNext stepsTutorialsLangChainBuild a RAG agent with LangChainCopy pageCopy page\\u200bOverview\\nOne of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.\\nThis tutorial will show how to build a simple Q&A application over an unstructured text data source. We will demonstrate:\\n\\nA RAG agent that executes searches with a simple tool. This is a good general-purpose implementation.\\nA two-step RAG chain that uses just a single LLM call per query. This is a fast and effective method for simple queries.\\n\\n\\u200bConcepts\\nWe will cover the following concepts:\\n\\n\\nIndexing: a pipeline for ingesting data from a source and indexing it. This usually happens in a separate process.\\n\\n\\nRetrieval and generation: the actual RAG process, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\\n\\n\\nOnce weâ€™ve indexed our data, we will use an agent as our orchestration framework to implement the retrieval and generation steps.\\nThe indexing portion of this tutorial will largely follow the semantic search tutorial.If your data is already available for search (i.e., you have a function to execute a search), or youâ€™re comfortable with the content from that tutorial, feel free to skip to the section on retrieval and generation\\n\\u200bPreview\\nIn this guide weâ€™ll build an app that answers questions about the websiteâ€™s content. The specific website we will use is the LLM Powered Autonomous Agents blog post by Lilian Weng, which allows us to ask questions about the contents of the post.\\nWe can create a simple indexing pipeline and RAG chain to do this in ~40 lines of code. See below for the full code snippet:\\nExpand for full code snippetCopyimport bs4\\nfrom langchain.agents import AgentState, create_agent\\nfrom langchain_community.document_loaders import WebBaseLoader\\nfrom langchain.messages import MessageLikeRepresentation\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\n\\n# Load and chunk contents of the blog\\nloader = WebBaseLoader(\\n    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\\n    bs_kwargs=dict(\\n        parse_only=bs4.SoupStrainer(\\n            class_=(\"post-content\", \"post-title\", \"post-header\")\\n        )\\n    ),\\n)\\ndocs = loader.load()\\n\\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\\nall_splits = text_splitter.split_documents(docs)\\n\\n# Index chunks\\n_ = vector_store.add_documents(documents=all_splits)\\n\\n# Construct a tool for retrieving context\\n@tool(response_format=\"content_and_artifact\")\\ndef retrieve_context(query: str):\\n    \"\"\"Retrieve information to help answer a query.\"\"\"\\n    retrieved_docs = vector_store.similarity_search(query, k=2)\\n    serialized = \"\\\\n\\\\n\".join(\\n        (f\"Source: {doc.metadata}\\\\nContent: {doc.page_content}\")\\n        for doc in retrieved_docs\\n    )\\n    return serialized, retrieved_docs\\n\\ntools = [retrieve_context]\\n# If desired, specify custom instructions\\nprompt = (\\n    \"You have access to a tool that retrieves context from a blog post. \"\\n    \"Use the tool to help answer user queries.\"\\n)\\nagent = create_agent(model, tools, system_prompt=prompt)\\nCopyquery = \"What is task decomposition?\"\\nfor step in agent.stream(\\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\\n    stream_mode=\"values\",\\n):\\n    step[\"messages\"][-1].pretty_print()\\nCopy================================ Human Message =================================\\n\\nWhat is task decomposition?\\n================================== Ai Message ==================================\\nTool Calls:\\n  retrieve_context (call_xTkJr8njRY0geNz43ZvGkX0R)\\n Call ID: call_xTkJr8njRY0geNz43ZvGkX0R\\n  Args:\\n    query: task decomposition\\n================================= Tool Message =================================\\nName: retrieve_context\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Task decomposition can be done by...\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Component One: Planning...\\n================================== Ai Message ==================================\\n\\nTask decomposition refers to...\\nCheck out the LangSmith trace.\\n\\u200bSetup\\n\\u200bInstallation\\nThis tutorial requires these langchain dependencies:\\npipuvCopypip install langchain langchain-text-splitters langchain-community bs4\\n\\nFor more details, see our Installation guide.\\n\\u200bLangSmith\\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith.\\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\\nCopyexport LANGSMITH_TRACING=\"true\"\\nexport LANGSMITH_API_KEY=\"...\"\\n\\nOr, set them in Python:\\nCopyimport getpass\\nimport os\\n\\nos.environ[\"LANGSMITH_TRACING\"] = \"true\"\\nos.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\\n\\n\\u200bComponents\\nWe will need to select three components from LangChainâ€™s suite of integrations.\\nSelect a chat model:\\n OpenAI Anthropic Azure Google Gemini AWS Bedrock HuggingFaceðŸ‘‰ Read the OpenAI chat model integration docsCopypip install -U \"langchain[openai]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\\n\\nmodel = init_chat_model(\"gpt-4.1\")\\nðŸ‘‰ Read the Anthropic chat model integration docsCopypip install -U \"langchain[anthropic]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"ANTHROPIC_API_KEY\"] = \"sk-...\"\\n\\nmodel = init_chat_model(\"claude-sonnet-4-5-20250929\")\\nðŸ‘‰ Read the Azure chat model integration docsCopypip install -U \"langchain[openai]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"AZURE_OPENAI_API_KEY\"] = \"...\"\\nos.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"...\"\\nos.environ[\"OPENAI_API_VERSION\"] = \"2025-03-01-preview\"\\n\\nmodel = init_chat_model(\\n    \"azure_openai:gpt-4.1\",\\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\\n)\\nðŸ‘‰ Read the Google GenAI chat model integration docsCopypip install -U \"langchain[google-genai]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"GOOGLE_API_KEY\"] = \"...\"\\n\\nmodel = init_chat_model(\"google_genai:gemini-2.5-flash-lite\")\\nðŸ‘‰ Read the AWS Bedrock chat model integration docsCopypip install -U \"langchain[aws]\"\\ninit_chat_modelModel ClassCopyfrom langchain.chat_models import init_chat_model\\n\\n# Follow the steps here to configure your credentials:\\n# https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html\\n\\nmodel = init_chat_model(\\n    \"anthropic.claude-3-5-sonnet-20240620-v1:0\",\\n    model_provider=\"bedrock_converse\",\\n)\\nðŸ‘‰ Read the HuggingFace chat model integration docsCopypip install -U \"langchain[huggingface]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"hf_...\"\\n\\nmodel = init_chat_model(\\n    \"microsoft/Phi-3-mini-4k-instruct\",\\n    model_provider=\"huggingface\",\\n    temperature=0.7,\\n    max_tokens=1024,\\n)\\n\\nSelect an embeddings model:\\n OpenAI Azure Google Gemini Google Vertex AWS HuggingFace Ollama Cohere MistralAI Nomic NVIDIA Voyage AI IBM watsonx Fake IsaacusCopypip install -U \"langchain-openai\"\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"OPENAI_API_KEY\"):\\n    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\\n\\nfrom langchain_openai import OpenAIEmbeddings\\n\\nembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\\nCopypip install -U \"langchain-openai\"\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"AZURE_OPENAI_API_KEY\"):\\n    os.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for Azure: \")\\n\\nfrom langchain_openai import AzureOpenAIEmbeddings\\n\\nembeddings = AzureOpenAIEmbeddings(\\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\\n)\\nCopypip install -qU langchain-google-genai\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"GOOGLE_API_KEY\"):\\n    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")\\n\\nfrom langchain_google_genai import GoogleGenerativeAIEmbeddings\\n\\nembeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")\\nCopypip install -qU langchain-google-vertexai\\nCopyfrom langchain_google_vertexai import VertexAIEmbeddings\\n\\nembeddings = VertexAIEmbeddings(model=\"text-embedding-005\")\\nCopypip install -qU langchain-aws\\nCopyfrom langchain_aws import BedrockEmbeddings\\n\\nembeddings = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v2:0\")\\nCopypip install -qU langchain-huggingface\\nCopyfrom langchain_huggingface import HuggingFaceEmbeddings\\n\\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\\nCopypip install -qU langchain-ollama\\nCopyfrom langchain_ollama import OllamaEmbeddings\\n\\nembeddings = OllamaEmbeddings(model=\"llama3\")\\nCopypip install -qU langchain-cohere\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"COHERE_API_KEY\"):\\n    os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Enter API key for Cohere: \")\\n\\nfrom langchain_cohere import CohereEmbeddings\\n\\nembeddings = CohereEmbeddings(model=\"embed-english-v3.0\")\\nCopypip install -qU langchain-mistralai\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"MISTRALAI_API_KEY\"):\\n    os.environ[\"MISTRALAI_API_KEY\"] = getpass.getpass(\"Enter API key for MistralAI: \")\\n\\nfrom langchain_mistralai import MistralAIEmbeddings\\n\\nembeddings = MistralAIEmbeddings(model=\"mistral-embed\")\\nCopypip install -qU langchain-nomic\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"NOMIC_API_KEY\"):\\n    os.environ[\"NOMIC_API_KEY\"] = getpass.getpass(\"Enter API key for Nomic: \")\\n\\nfrom langchain_nomic import NomicEmbeddings\\n\\nembeddings = NomicEmbeddings(model=\"nomic-embed-text-v1.5\")\\nCopypip install -qU langchain-nvidia-ai-endpoints\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"NVIDIA_API_KEY\"):\\n    os.environ[\"NVIDIA_API_KEY\"] = getpass.getpass(\"Enter API key for NVIDIA: \")\\n\\nfrom langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\\n\\nembeddings = NVIDIAEmbeddings(model=\"NV-Embed-QA\")\\nCopypip install -qU langchain-voyageai\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"VOYAGE_API_KEY\"):\\n    os.environ[\"VOYAGE_API_KEY\"] = getpass.getpass(\"Enter API key for Voyage AI: \")\\n\\nfrom langchain-voyageai import VoyageAIEmbeddings\\n\\nembeddings = VoyageAIEmbeddings(model=\"voyage-3\")\\nCopypip install -qU langchain-ibm\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"WATSONX_APIKEY\"):\\n    os.environ[\"WATSONX_APIKEY\"] = getpass.getpass(\"Enter API key for IBM watsonx: \")\\n\\nfrom langchain_ibm import WatsonxEmbeddings\\n\\nembeddings = WatsonxEmbeddings(\\n    model_id=\"ibm/slate-125m-english-rtrvr\",\\n    url=\"https://us-south.ml.cloud.ibm.com\",\\n    project_id=\"<WATSONX PROJECT_ID>\",\\n)\\nCopypip install -qU langchain-core\\nCopyfrom langchain_core.embeddings import DeterministicFakeEmbedding\\n\\nembeddings = DeterministicFakeEmbedding(size=4096)\\nCopypip install -qU langchain-isaacus\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"ISAACUS_API_KEY\"):\\nos.environ[\"ISAACUS_API_KEY\"] = getpass.getpass(\"Enter API key for Isaacus: \")\\n\\nfrom langchain_isaacus import IsaacusEmbeddings\\n\\nembeddings = IsaacusEmbeddings(model=\"kanon-2-embedder\")\\n\\nSelect a vector store:\\n In-memory Amazon OpenSearch AstraDB Chroma FAISS Milvus MongoDB PGVector PGVectorStore Pinecone QdrantCopypip install -U \"langchain-core\"\\nCopyfrom langchain_core.vectorstores import InMemoryVectorStore\\n\\nvector_store = InMemoryVectorStore(embeddings)\\nCopypip install -qU  boto3\\nCopyfrom opensearchpy import RequestsHttpConnection\\n\\nservice = \"es\"  # must set the service as \\'es\\'\\nregion = \"us-east-2\"\\ncredentials = boto3.Session(\\n    aws_access_key_id=\"xxxxxx\", aws_secret_access_key=\"xxxxx\"\\n).get_credentials()\\nawsauth = AWS4Auth(\"xxxxx\", \"xxxxxx\", region, service, session_token=credentials.token)\\n\\nvector_store = OpenSearchVectorSearch.from_documents(\\n    docs,\\n    embeddings,\\n    opensearch_url=\"host url\",\\n    http_auth=awsauth,\\n    timeout=300,\\n    use_ssl=True,\\n    verify_certs=True,\\n    connection_class=RequestsHttpConnection,\\n    index_name=\"test-index\",\\n)\\nCopypip install -U \"langchain-astradb\"\\nCopyfrom langchain_astradb import AstraDBVectorStore\\n\\nvector_store = AstraDBVectorStore(\\n    embedding=embeddings,\\n    api_endpoint=ASTRA_DB_API_ENDPOINT,\\n    collection_name=\"astra_vector_langchain\",\\n    token=ASTRA_DB_APPLICATION_TOKEN,\\n    namespace=ASTRA_DB_NAMESPACE,\\n)\\nCopypip install -qU langchain-chroma\\nCopyfrom langchain_chroma import Chroma\\n\\nvector_store = Chroma(\\n    collection_name=\"example_collection\",\\n    embedding_function=embeddings,\\n    persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not necessary\\n)\\nCopypip install -qU langchain-community faiss-cpu\\nCopyimport faiss\\nfrom langchain_community.docstore.in_memory import InMemoryDocstore\\nfrom langchain_community.vectorstores import FAISS\\n\\nembedding_dim = len(embeddings.embed_query(\"hello world\"))\\nindex = faiss.IndexFlatL2(embedding_dim)\\n\\nvector_store = FAISS(\\n    embedding_function=embeddings,\\n    index=index,\\n    docstore=InMemoryDocstore(),\\n    index_to_docstore_id={},\\n)\\nCopypip install -qU langchain-milvus\\nCopyfrom langchain_milvus import Milvus\\n\\nURI = \"./milvus_example.db\"\\n\\nvector_store = Milvus(\\n    embedding_function=embeddings,\\n    connection_args={\"uri\": URI},\\n    index_params={\"index_type\": \"FLAT\", \"metric_type\": \"L2\"},\\n)\\nCopypip install -qU langchain-mongodb\\nCopyfrom langchain_mongodb import MongoDBAtlasVectorSearch\\n\\nvector_store = MongoDBAtlasVectorSearch(\\n    embedding=embeddings,\\n    collection=MONGODB_COLLECTION,\\n    index_name=ATLAS_VECTOR_SEARCH_INDEX_NAME,\\n    relevance_score_fn=\"cosine\",\\n)\\nCopypip install -qU langchain-postgres\\nCopyfrom langchain_postgres import PGVector\\n\\nvector_store = PGVector(\\n    embeddings=embeddings,\\n    collection_name=\"my_docs\",\\n    connection=\"postgresql+psycopg://...\",\\n)\\nCopypip install -qU langchain-postgres\\nCopyfrom langchain_postgres import PGEngine, PGVectorStore\\n\\npg_engine = PGEngine.from_connection_string(\\n    url=\"postgresql+psycopg://...\"\\n)\\n\\nvector_store = PGVectorStore.create_sync(\\n    engine=pg_engine,\\n    table_name=\\'test_table\\',\\n    embedding_service=embedding\\n)\\nCopypip install -qU langchain-pinecone\\nCopyfrom langchain_pinecone import PineconeVectorStore\\nfrom pinecone import Pinecone\\n\\npc = Pinecone(api_key=...)\\nindex = pc.Index(index_name)\\n\\nvector_store = PineconeVectorStore(embedding=embeddings, index=index)\\nCopypip install -qU langchain-qdrant\\nCopyfrom qdrant_client.models import Distance, VectorParams\\nfrom langchain_qdrant import QdrantVectorStore\\nfrom qdrant_client import QdrantClient\\n\\nclient = QdrantClient(\":memory:\")\\n\\nvector_size = len(embeddings.embed_query(\"sample text\"))\\n\\nif not client.collection_exists(\"test\"):\\n    client.create_collection(\\n        collection_name=\"test\",\\n        vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE)\\n    )\\nvector_store = QdrantVectorStore(\\n    client=client,\\n    collection_name=\"test\",\\n    embedding=embeddings,\\n)\\n\\n\\u200b1. Indexing\\nThis section is an abbreviated version of the content in the semantic search tutorial.If your data is already indexed and available for search (i.e., you have a function to execute a search), or if youâ€™re comfortable with document loaders, embeddings, and vector stores, feel free to skip to the next section on retrieval and generation.\\nIndexing commonly works as follows:\\n\\nLoad: First we need to load our data. This is done with Document Loaders.\\nSplit: Text splitters break large Documents into smaller chunks. This is useful both for indexing data and passing it into a model, as large chunks are harder to search over and wonâ€™t fit in a modelâ€™s finite context window.\\nStore: We need somewhere to store and index our splits, so that they can be searched over later. This is often done using a VectorStore and Embeddings model.\\n\\n\\n\\u200bLoading documents\\nWe need to first load the blog post contents. We can use DocumentLoaders for this, which are objects that load in data from a source and return a list of Document objects.\\nIn this case weâ€™ll use the WebBaseLoader, which uses urllib to load HTML from web URLs and BeautifulSoup to parse it to text. We can customize the HTML -> text parsing by passing in parameters into the BeautifulSoup parser via bs_kwargs (see BeautifulSoup docs). In this case only HTML tags with class â€œpost-contentâ€, â€œpost-titleâ€, or â€œpost-headerâ€ are relevant, so weâ€™ll remove all others.\\nCopyimport bs4\\nfrom langchain_community.document_loaders import WebBaseLoader\\n\\n# Only keep post title, headers, and content from the full HTML.\\nbs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\\nloader = WebBaseLoader(\\n    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\\n    bs_kwargs={\"parse_only\": bs4_strainer},\\n)\\ndocs = loader.load()\\n\\nassert len(docs) == 1\\nprint(f\"Total characters: {len(docs[0].page_content)}\")\\n\\nCopyTotal characters: 43131\\n\\nCopyprint(docs[0].page_content[:500])\\n\\nCopy      LLM Powered Autonomous Agents\\n\\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn\\n\\nGo deeper\\nDocumentLoader: Object that loads data from a source as list of Documents.\\n\\nIntegrations: 160+ integrations to choose from.\\nBaseLoader: API reference for the base interface.\\n\\n\\u200bSplitting documents\\nOur loaded document is over 42k characters which is too long to fit into the context window of many models. Even for those models that could fit the full post in their context window, models can struggle to find information in very long inputs.\\nTo handle this weâ€™ll split the Document into chunks for embedding and vector storage. This should help us retrieve only the most relevant parts of the blog post at run time.\\nAs in the semantic search tutorial, we use a RecursiveCharacterTextSplitter, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases.\\nCopyfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\n\\ntext_splitter = RecursiveCharacterTextSplitter(\\n    chunk_size=1000,  # chunk size (characters)\\n    chunk_overlap=200,  # chunk overlap (characters)\\n    add_start_index=True,  # track index in original document\\n)\\nall_splits = text_splitter.split_documents(docs)\\n\\nprint(f\"Split blog post into {len(all_splits)} sub-documents.\")\\n\\nCopySplit blog post into 66 sub-documents.\\n\\nGo deeper\\nTextSplitter: Object that splits a list of Document objects into smaller\\nchunks for storage and retrieval.\\n\\nIntegrations\\nInterface: API reference for the base interface.\\n\\n\\u200bStoring documents\\nNow we need to index our 66 text chunks so that we can search over them at runtime. Following the semantic search tutorial, our approach is to embed the contents of each document split and insert these embeddings into a vector store. Given an input query, we can then use vector search to retrieve relevant documents.\\nWe can embed and store all of our document splits in a single command using the vector store and embeddings model selected at the start of the tutorial.\\nCopydocument_ids = vector_store.add_documents(documents=all_splits)\\n\\nprint(document_ids[:3])\\n\\nCopy[\\'07c18af6-ad58-479a-bfb1-d508033f9c64\\', \\'9000bf8e-1993-446f-8d4d-f4e507ba4b8f\\', \\'ba3b5d14-bed9-4f5f-88be-44c88aedc2e6\\']\\n\\nGo deeper\\nEmbeddings: Wrapper around a text embedding model, used for converting text to embeddings.\\n\\nIntegrations: 30+ integrations to choose from.\\nInterface: API reference for the base interface.\\n\\nVectorStore: Wrapper around a vector database, used for storing and querying embeddings.\\n\\nIntegrations: 40+ integrations to choose from.\\nInterface: API reference for the base interface.\\n\\nThis completes the Indexing portion of the pipeline. At this point we have a query-able vector store containing the chunked contents of our blog post. Given a user question, we should ideally be able to return the snippets of the blog post that answer the question.\\n\\u200b2. Retrieval and Generation\\nRAG applications commonly work as follows:\\n\\nRetrieve: Given a user input, relevant splits are retrieved from storage using a Retriever.\\nGenerate: A model produces an answer using a prompt that includes both the question with the retrieved data\\n\\n\\nNow letâ€™s write the actual application logic. We want to create a simple application that takes a user question, searches for documents relevant to that question, passes the retrieved documents and initial question to a model, and returns an answer.\\nWe will demonstrate:\\n\\nA RAG agent that executes searches with a simple tool. This is a good general-purpose implementation.\\nA two-step RAG chain that uses just a single LLM call per query. This is a fast and effective method for simple queries.\\n\\n\\u200bRAG agents\\nOne formulation of a RAG application is as a simple agent with a tool that retrieves information. We can assemble a minimal RAG agent by implementing a tool that wraps our vector store:\\nCopyfrom langchain.tools import tool\\n\\n@tool(response_format=\"content_and_artifact\")\\ndef retrieve_context(query: str):\\n    \"\"\"Retrieve information to help answer a query.\"\"\"\\n    retrieved_docs = vector_store.similarity_search(query, k=2)\\n    serialized = \"\\\\n\\\\n\".join(\\n        (f\"Source: {doc.metadata}\\\\nContent: {doc.page_content}\")\\n        for doc in retrieved_docs\\n    )\\n    return serialized, retrieved_docs\\n\\nHere we use the tool decorator to configure the tool to attach raw documents as artifacts to each ToolMessage. This will let us access document metadata in our application, separate from the stringified representation that is sent to the model.\\nRetrieval tools are not limited to a single string query argument, as in the above example. You can\\nforce the LLM to specify additional search parameters by adding argumentsâ€” for example, a category:Copyfrom typing import Literal\\n\\ndef retrieve_context(query: str, section: Literal[\"beginning\", \"middle\", \"end\"]):\\n\\nGiven our tool, we can construct the agent:\\nCopyfrom langchain.agents import create_agent\\n\\n\\ntools = [retrieve_context]\\n# If desired, specify custom instructions\\nprompt = (\\n    \"You have access to a tool that retrieves context from a blog post. \"\\n    \"Use the tool to help answer user queries.\"\\n)\\nagent = create_agent(model, tools, system_prompt=prompt)\\n\\nLetâ€™s test this out. We construct a question that would typically require an iterative sequence of retrieval steps to answer:\\nCopyquery = (\\n    \"What is the standard method for Task Decomposition?\\\\n\\\\n\"\\n    \"Once you get the answer, look up common extensions of that method.\"\\n)\\n\\nfor event in agent.stream(\\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\\n    stream_mode=\"values\",\\n):\\n    event[\"messages\"][-1].pretty_print()\\n\\nCopy================================ Human Message =================================\\n\\nWhat is the standard method for Task Decomposition?\\n\\nOnce you get the answer, look up common extensions of that method.\\n================================== Ai Message ==================================\\nTool Calls:\\n  retrieve_context (call_d6AVxICMPQYwAKj9lgH4E337)\\n Call ID: call_d6AVxICMPQYwAKj9lgH4E337\\n  Args:\\n    query: standard method for Task Decomposition\\n================================= Tool Message =================================\\nName: retrieve_context\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Task decomposition can be done...\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Component One: Planning...\\n================================== Ai Message ==================================\\nTool Calls:\\n  retrieve_context (call_0dbMOw7266jvETbXWn4JqWpR)\\n Call ID: call_0dbMOw7266jvETbXWn4JqWpR\\n  Args:\\n    query: common extensions of the standard method for Task Decomposition\\n================================= Tool Message =================================\\nName: retrieve_context\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Task decomposition can be done...\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Component One: Planning...\\n================================== Ai Message ==================================\\n\\nThe standard method for Task Decomposition often used is the Chain of Thought (CoT)...\\n\\nNote that the agent:\\n\\nGenerates a query to search for a standard method for task decomposition;\\nReceiving the answer, generates a second query to search for common extensions of it;\\nHaving received all necessary context, answers the question.\\n\\nWe can see the full sequence of steps, along with latency and other metadata, in the LangSmith trace.\\nYou can add a deeper level of control and customization using the LangGraph framework directlyâ€” for example, you can add steps to grade document relevance and rewrite search queries. Check out LangGraphâ€™s Agentic RAG tutorial for more advanced formulations.\\n\\u200bRAG chains\\nIn the above agentic RAG formulation we allow the LLM to use its discretion in generating a tool call to help answer user queries. This is a good general-purpose solution, but comes with some trade-offs:\\nâœ… Benefitsâš ï¸ DrawbacksSearch only when needed â€“ The LLM can handle greetings, follow-ups, and simple queries without triggering unnecessary searches.Two inference calls â€“ When a search is performed, it requires one call to generate the query and another to produce the final response.Contextual search queries â€“ By treating search as a tool with a query input, the LLM crafts its own queries that incorporate conversational context.Reduced control â€“ The LLM may skip searches when they are actually needed, or issue extra searches when unnecessary.Multiple searches allowed â€“ The LLM can execute several searches in support of a single user query.\\nAnother common approach is a two-step chain, in which we always run a search (potentially using the raw user query) and incorporate the result as context for a single LLM query. This results in a single inference call per query, buying reduced latency at the expense of flexibility.\\nIn this approach we no longer call the model in a loop, but instead make a single pass.\\nWe can implement this chain by removing tools from the agent and instead incorporating the retrieval step into a custom prompt:\\nCopyfrom langchain.agents.middleware import dynamic_prompt, ModelRequest\\n\\n@dynamic_prompt\\ndef prompt_with_context(request: ModelRequest) -> str:\\n    \"\"\"Inject context into state messages.\"\"\"\\n    last_query = request.state[\"messages\"][-1].text\\n    retrieved_docs = vector_store.similarity_search(last_query)\\n\\n    docs_content = \"\\\\n\\\\n\".join(doc.page_content for doc in retrieved_docs)\\n\\n    system_message = (\\n        \"You are a helpful assistant. Use the following context in your response:\"\\n        f\"\\\\n\\\\n{docs_content}\"\\n    )\\n\\n    return system_message\\n\\n\\nagent = create_agent(model, tools=[], middleware=[prompt_with_context])\\n\\nLetâ€™s try this out:\\nCopyquery = \"What is task decomposition?\"\\nfor step in agent.stream(\\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\\n    stream_mode=\"values\",\\n):\\n    step[\"messages\"][-1].pretty_print()\\n\\nCopy================================ Human Message =================================\\n\\nWhat is task decomposition?\\n================================== Ai Message ==================================\\n\\nTask decomposition is...\\n\\nIn the LangSmith trace we can see the retrieved context incorporated into the model prompt.\\nThis is a fast and effective method for simple queries in constrained settings, when we typically do want to run user queries through semantic search to pull additional context.\\nReturning source documentsThe above RAG chain incorporates retrieved context into a single system message for that run.As in the agentic RAG formulation, we sometimes want to include raw source documents in the application state to have access to document metadata. We can do this for the two-step chain case by:\\nAdding a key to the state to store the retrieved documents\\nAdding a new node via a pre-model hook to populate that key (as well as inject the context).\\nCopyfrom typing import Any\\nfrom langchain_core.documents import Document\\nfrom langchain.agents.middleware import AgentMiddleware, AgentState\\n\\n\\nclass State(AgentState):\\n    context: list[Document]\\n\\n\\nclass RetrieveDocumentsMiddleware(AgentMiddleware[State]):\\n    state_schema = State\\n\\n    def before_model(self, state: AgentState) -> dict[str, Any] | None:\\n        last_message = state[\"messages\"][-1]\\n        retrieved_docs = vector_store.similarity_search(last_message.text)\\n\\n        docs_content = \"\\\\n\\\\n\".join(doc.page_content for doc in retrieved_docs)\\n\\n        augmented_message_content = (\\n            f\"{last_message.text}\\\\n\\\\n\"\\n            \"Use the following context to answer the query:\\\\n\"\\n            f\"{docs_content}\"\\n        )\\n        return {\\n            \"messages\": [last_message.model_copy(update={\"content\": augmented_message_content})],\\n            \"context\": retrieved_docs,\\n        }\\n\\n\\nagent = create_agent(\\n    model,\\n    tools=[],\\n    middleware=[RetrieveDocumentsMiddleware()],\\n)\\n\\n\\u200bNext steps\\nNow that weâ€™ve implemented a simple RAG application via create_agent, we can easily incorporate new features and go deeper:\\n\\nStream tokens and other information for responsive user experiences\\nAdd conversational memory to support multi-turn interactions\\nAdd long-term memory to support memory across conversational threads\\nAdd structured responses\\nDeploy your application with LangSmith Deployment\\n\\n\\nEdit this page on GitHub or file an issue.\\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.Was this page helpful?YesNoBuild a semantic search engine with LangChainPreviousBuild a SQL agentNextâŒ˜IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by')],\n",
       " [Document(metadata={'source': 'https://python.langchain.com/docs/tutorials/qa_chat_history/', 'title': 'Build a RAG agent with LangChain - Docs by LangChain', 'language': 'en'}, page_content='Build a RAG agent with LangChain - Docs by LangChainSkip to main contentDocs by LangChain home pageLangChain + LangGraphSearch...âŒ˜KAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationLangChainBuild a RAG agent with LangChainLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonLearnTutorialsLangChainSemantic searchRAG agentSQL agentVoice agentMulti-agentLangGraphConceptual overviewsComponent architectureMemoryContextGraph APIFunctional APIAdditional resourcesLangChain AcademyCase studiesGet helpOn this pageOverviewConceptsPreviewSetupInstallationLangSmithComponents1. IndexingLoading documentsSplitting documentsStoring documents2. Retrieval and GenerationRAG agentsRAG chainsNext stepsTutorialsLangChainBuild a RAG agent with LangChainCopy pageCopy page\\u200bOverview\\nOne of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.\\nThis tutorial will show how to build a simple Q&A application over an unstructured text data source. We will demonstrate:\\n\\nA RAG agent that executes searches with a simple tool. This is a good general-purpose implementation.\\nA two-step RAG chain that uses just a single LLM call per query. This is a fast and effective method for simple queries.\\n\\n\\u200bConcepts\\nWe will cover the following concepts:\\n\\n\\nIndexing: a pipeline for ingesting data from a source and indexing it. This usually happens in a separate process.\\n\\n\\nRetrieval and generation: the actual RAG process, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\\n\\n\\nOnce weâ€™ve indexed our data, we will use an agent as our orchestration framework to implement the retrieval and generation steps.\\nThe indexing portion of this tutorial will largely follow the semantic search tutorial.If your data is already available for search (i.e., you have a function to execute a search), or youâ€™re comfortable with the content from that tutorial, feel free to skip to the section on retrieval and generation\\n\\u200bPreview\\nIn this guide weâ€™ll build an app that answers questions about the websiteâ€™s content. The specific website we will use is the LLM Powered Autonomous Agents blog post by Lilian Weng, which allows us to ask questions about the contents of the post.\\nWe can create a simple indexing pipeline and RAG chain to do this in ~40 lines of code. See below for the full code snippet:\\nExpand for full code snippetCopyimport bs4\\nfrom langchain.agents import AgentState, create_agent\\nfrom langchain_community.document_loaders import WebBaseLoader\\nfrom langchain.messages import MessageLikeRepresentation\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\n\\n# Load and chunk contents of the blog\\nloader = WebBaseLoader(\\n    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\\n    bs_kwargs=dict(\\n        parse_only=bs4.SoupStrainer(\\n            class_=(\"post-content\", \"post-title\", \"post-header\")\\n        )\\n    ),\\n)\\ndocs = loader.load()\\n\\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\\nall_splits = text_splitter.split_documents(docs)\\n\\n# Index chunks\\n_ = vector_store.add_documents(documents=all_splits)\\n\\n# Construct a tool for retrieving context\\n@tool(response_format=\"content_and_artifact\")\\ndef retrieve_context(query: str):\\n    \"\"\"Retrieve information to help answer a query.\"\"\"\\n    retrieved_docs = vector_store.similarity_search(query, k=2)\\n    serialized = \"\\\\n\\\\n\".join(\\n        (f\"Source: {doc.metadata}\\\\nContent: {doc.page_content}\")\\n        for doc in retrieved_docs\\n    )\\n    return serialized, retrieved_docs\\n\\ntools = [retrieve_context]\\n# If desired, specify custom instructions\\nprompt = (\\n    \"You have access to a tool that retrieves context from a blog post. \"\\n    \"Use the tool to help answer user queries.\"\\n)\\nagent = create_agent(model, tools, system_prompt=prompt)\\nCopyquery = \"What is task decomposition?\"\\nfor step in agent.stream(\\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\\n    stream_mode=\"values\",\\n):\\n    step[\"messages\"][-1].pretty_print()\\nCopy================================ Human Message =================================\\n\\nWhat is task decomposition?\\n================================== Ai Message ==================================\\nTool Calls:\\n  retrieve_context (call_xTkJr8njRY0geNz43ZvGkX0R)\\n Call ID: call_xTkJr8njRY0geNz43ZvGkX0R\\n  Args:\\n    query: task decomposition\\n================================= Tool Message =================================\\nName: retrieve_context\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Task decomposition can be done by...\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Component One: Planning...\\n================================== Ai Message ==================================\\n\\nTask decomposition refers to...\\nCheck out the LangSmith trace.\\n\\u200bSetup\\n\\u200bInstallation\\nThis tutorial requires these langchain dependencies:\\npipuvCopypip install langchain langchain-text-splitters langchain-community bs4\\n\\nFor more details, see our Installation guide.\\n\\u200bLangSmith\\nMany of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith.\\nAfter you sign up at the link above, make sure to set your environment variables to start logging traces:\\nCopyexport LANGSMITH_TRACING=\"true\"\\nexport LANGSMITH_API_KEY=\"...\"\\n\\nOr, set them in Python:\\nCopyimport getpass\\nimport os\\n\\nos.environ[\"LANGSMITH_TRACING\"] = \"true\"\\nos.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\\n\\n\\u200bComponents\\nWe will need to select three components from LangChainâ€™s suite of integrations.\\nSelect a chat model:\\n OpenAI Anthropic Azure Google Gemini AWS Bedrock HuggingFaceðŸ‘‰ Read the OpenAI chat model integration docsCopypip install -U \"langchain[openai]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\\n\\nmodel = init_chat_model(\"gpt-4.1\")\\nðŸ‘‰ Read the Anthropic chat model integration docsCopypip install -U \"langchain[anthropic]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"ANTHROPIC_API_KEY\"] = \"sk-...\"\\n\\nmodel = init_chat_model(\"claude-sonnet-4-5-20250929\")\\nðŸ‘‰ Read the Azure chat model integration docsCopypip install -U \"langchain[openai]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"AZURE_OPENAI_API_KEY\"] = \"...\"\\nos.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"...\"\\nos.environ[\"OPENAI_API_VERSION\"] = \"2025-03-01-preview\"\\n\\nmodel = init_chat_model(\\n    \"azure_openai:gpt-4.1\",\\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\\n)\\nðŸ‘‰ Read the Google GenAI chat model integration docsCopypip install -U \"langchain[google-genai]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"GOOGLE_API_KEY\"] = \"...\"\\n\\nmodel = init_chat_model(\"google_genai:gemini-2.5-flash-lite\")\\nðŸ‘‰ Read the AWS Bedrock chat model integration docsCopypip install -U \"langchain[aws]\"\\ninit_chat_modelModel ClassCopyfrom langchain.chat_models import init_chat_model\\n\\n# Follow the steps here to configure your credentials:\\n# https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html\\n\\nmodel = init_chat_model(\\n    \"anthropic.claude-3-5-sonnet-20240620-v1:0\",\\n    model_provider=\"bedrock_converse\",\\n)\\nðŸ‘‰ Read the HuggingFace chat model integration docsCopypip install -U \"langchain[huggingface]\"\\ninit_chat_modelModel ClassCopyimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"hf_...\"\\n\\nmodel = init_chat_model(\\n    \"microsoft/Phi-3-mini-4k-instruct\",\\n    model_provider=\"huggingface\",\\n    temperature=0.7,\\n    max_tokens=1024,\\n)\\n\\nSelect an embeddings model:\\n OpenAI Azure Google Gemini Google Vertex AWS HuggingFace Ollama Cohere MistralAI Nomic NVIDIA Voyage AI IBM watsonx Fake IsaacusCopypip install -U \"langchain-openai\"\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"OPENAI_API_KEY\"):\\n    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\\n\\nfrom langchain_openai import OpenAIEmbeddings\\n\\nembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\\nCopypip install -U \"langchain-openai\"\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"AZURE_OPENAI_API_KEY\"):\\n    os.environ[\"AZURE_OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for Azure: \")\\n\\nfrom langchain_openai import AzureOpenAIEmbeddings\\n\\nembeddings = AzureOpenAIEmbeddings(\\n    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\\n    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\\n)\\nCopypip install -qU langchain-google-genai\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"GOOGLE_API_KEY\"):\\n    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")\\n\\nfrom langchain_google_genai import GoogleGenerativeAIEmbeddings\\n\\nembeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")\\nCopypip install -qU langchain-google-vertexai\\nCopyfrom langchain_google_vertexai import VertexAIEmbeddings\\n\\nembeddings = VertexAIEmbeddings(model=\"text-embedding-005\")\\nCopypip install -qU langchain-aws\\nCopyfrom langchain_aws import BedrockEmbeddings\\n\\nembeddings = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v2:0\")\\nCopypip install -qU langchain-huggingface\\nCopyfrom langchain_huggingface import HuggingFaceEmbeddings\\n\\nembeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\\nCopypip install -qU langchain-ollama\\nCopyfrom langchain_ollama import OllamaEmbeddings\\n\\nembeddings = OllamaEmbeddings(model=\"llama3\")\\nCopypip install -qU langchain-cohere\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"COHERE_API_KEY\"):\\n    os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Enter API key for Cohere: \")\\n\\nfrom langchain_cohere import CohereEmbeddings\\n\\nembeddings = CohereEmbeddings(model=\"embed-english-v3.0\")\\nCopypip install -qU langchain-mistralai\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"MISTRALAI_API_KEY\"):\\n    os.environ[\"MISTRALAI_API_KEY\"] = getpass.getpass(\"Enter API key for MistralAI: \")\\n\\nfrom langchain_mistralai import MistralAIEmbeddings\\n\\nembeddings = MistralAIEmbeddings(model=\"mistral-embed\")\\nCopypip install -qU langchain-nomic\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"NOMIC_API_KEY\"):\\n    os.environ[\"NOMIC_API_KEY\"] = getpass.getpass(\"Enter API key for Nomic: \")\\n\\nfrom langchain_nomic import NomicEmbeddings\\n\\nembeddings = NomicEmbeddings(model=\"nomic-embed-text-v1.5\")\\nCopypip install -qU langchain-nvidia-ai-endpoints\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"NVIDIA_API_KEY\"):\\n    os.environ[\"NVIDIA_API_KEY\"] = getpass.getpass(\"Enter API key for NVIDIA: \")\\n\\nfrom langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\\n\\nembeddings = NVIDIAEmbeddings(model=\"NV-Embed-QA\")\\nCopypip install -qU langchain-voyageai\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"VOYAGE_API_KEY\"):\\n    os.environ[\"VOYAGE_API_KEY\"] = getpass.getpass(\"Enter API key for Voyage AI: \")\\n\\nfrom langchain-voyageai import VoyageAIEmbeddings\\n\\nembeddings = VoyageAIEmbeddings(model=\"voyage-3\")\\nCopypip install -qU langchain-ibm\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"WATSONX_APIKEY\"):\\n    os.environ[\"WATSONX_APIKEY\"] = getpass.getpass(\"Enter API key for IBM watsonx: \")\\n\\nfrom langchain_ibm import WatsonxEmbeddings\\n\\nembeddings = WatsonxEmbeddings(\\n    model_id=\"ibm/slate-125m-english-rtrvr\",\\n    url=\"https://us-south.ml.cloud.ibm.com\",\\n    project_id=\"<WATSONX PROJECT_ID>\",\\n)\\nCopypip install -qU langchain-core\\nCopyfrom langchain_core.embeddings import DeterministicFakeEmbedding\\n\\nembeddings = DeterministicFakeEmbedding(size=4096)\\nCopypip install -qU langchain-isaacus\\nCopyimport getpass\\nimport os\\n\\nif not os.environ.get(\"ISAACUS_API_KEY\"):\\nos.environ[\"ISAACUS_API_KEY\"] = getpass.getpass(\"Enter API key for Isaacus: \")\\n\\nfrom langchain_isaacus import IsaacusEmbeddings\\n\\nembeddings = IsaacusEmbeddings(model=\"kanon-2-embedder\")\\n\\nSelect a vector store:\\n In-memory Amazon OpenSearch AstraDB Chroma FAISS Milvus MongoDB PGVector PGVectorStore Pinecone QdrantCopypip install -U \"langchain-core\"\\nCopyfrom langchain_core.vectorstores import InMemoryVectorStore\\n\\nvector_store = InMemoryVectorStore(embeddings)\\nCopypip install -qU  boto3\\nCopyfrom opensearchpy import RequestsHttpConnection\\n\\nservice = \"es\"  # must set the service as \\'es\\'\\nregion = \"us-east-2\"\\ncredentials = boto3.Session(\\n    aws_access_key_id=\"xxxxxx\", aws_secret_access_key=\"xxxxx\"\\n).get_credentials()\\nawsauth = AWS4Auth(\"xxxxx\", \"xxxxxx\", region, service, session_token=credentials.token)\\n\\nvector_store = OpenSearchVectorSearch.from_documents(\\n    docs,\\n    embeddings,\\n    opensearch_url=\"host url\",\\n    http_auth=awsauth,\\n    timeout=300,\\n    use_ssl=True,\\n    verify_certs=True,\\n    connection_class=RequestsHttpConnection,\\n    index_name=\"test-index\",\\n)\\nCopypip install -U \"langchain-astradb\"\\nCopyfrom langchain_astradb import AstraDBVectorStore\\n\\nvector_store = AstraDBVectorStore(\\n    embedding=embeddings,\\n    api_endpoint=ASTRA_DB_API_ENDPOINT,\\n    collection_name=\"astra_vector_langchain\",\\n    token=ASTRA_DB_APPLICATION_TOKEN,\\n    namespace=ASTRA_DB_NAMESPACE,\\n)\\nCopypip install -qU langchain-chroma\\nCopyfrom langchain_chroma import Chroma\\n\\nvector_store = Chroma(\\n    collection_name=\"example_collection\",\\n    embedding_function=embeddings,\\n    persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not necessary\\n)\\nCopypip install -qU langchain-community faiss-cpu\\nCopyimport faiss\\nfrom langchain_community.docstore.in_memory import InMemoryDocstore\\nfrom langchain_community.vectorstores import FAISS\\n\\nembedding_dim = len(embeddings.embed_query(\"hello world\"))\\nindex = faiss.IndexFlatL2(embedding_dim)\\n\\nvector_store = FAISS(\\n    embedding_function=embeddings,\\n    index=index,\\n    docstore=InMemoryDocstore(),\\n    index_to_docstore_id={},\\n)\\nCopypip install -qU langchain-milvus\\nCopyfrom langchain_milvus import Milvus\\n\\nURI = \"./milvus_example.db\"\\n\\nvector_store = Milvus(\\n    embedding_function=embeddings,\\n    connection_args={\"uri\": URI},\\n    index_params={\"index_type\": \"FLAT\", \"metric_type\": \"L2\"},\\n)\\nCopypip install -qU langchain-mongodb\\nCopyfrom langchain_mongodb import MongoDBAtlasVectorSearch\\n\\nvector_store = MongoDBAtlasVectorSearch(\\n    embedding=embeddings,\\n    collection=MONGODB_COLLECTION,\\n    index_name=ATLAS_VECTOR_SEARCH_INDEX_NAME,\\n    relevance_score_fn=\"cosine\",\\n)\\nCopypip install -qU langchain-postgres\\nCopyfrom langchain_postgres import PGVector\\n\\nvector_store = PGVector(\\n    embeddings=embeddings,\\n    collection_name=\"my_docs\",\\n    connection=\"postgresql+psycopg://...\",\\n)\\nCopypip install -qU langchain-postgres\\nCopyfrom langchain_postgres import PGEngine, PGVectorStore\\n\\npg_engine = PGEngine.from_connection_string(\\n    url=\"postgresql+psycopg://...\"\\n)\\n\\nvector_store = PGVectorStore.create_sync(\\n    engine=pg_engine,\\n    table_name=\\'test_table\\',\\n    embedding_service=embedding\\n)\\nCopypip install -qU langchain-pinecone\\nCopyfrom langchain_pinecone import PineconeVectorStore\\nfrom pinecone import Pinecone\\n\\npc = Pinecone(api_key=...)\\nindex = pc.Index(index_name)\\n\\nvector_store = PineconeVectorStore(embedding=embeddings, index=index)\\nCopypip install -qU langchain-qdrant\\nCopyfrom qdrant_client.models import Distance, VectorParams\\nfrom langchain_qdrant import QdrantVectorStore\\nfrom qdrant_client import QdrantClient\\n\\nclient = QdrantClient(\":memory:\")\\n\\nvector_size = len(embeddings.embed_query(\"sample text\"))\\n\\nif not client.collection_exists(\"test\"):\\n    client.create_collection(\\n        collection_name=\"test\",\\n        vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE)\\n    )\\nvector_store = QdrantVectorStore(\\n    client=client,\\n    collection_name=\"test\",\\n    embedding=embeddings,\\n)\\n\\n\\u200b1. Indexing\\nThis section is an abbreviated version of the content in the semantic search tutorial.If your data is already indexed and available for search (i.e., you have a function to execute a search), or if youâ€™re comfortable with document loaders, embeddings, and vector stores, feel free to skip to the next section on retrieval and generation.\\nIndexing commonly works as follows:\\n\\nLoad: First we need to load our data. This is done with Document Loaders.\\nSplit: Text splitters break large Documents into smaller chunks. This is useful both for indexing data and passing it into a model, as large chunks are harder to search over and wonâ€™t fit in a modelâ€™s finite context window.\\nStore: We need somewhere to store and index our splits, so that they can be searched over later. This is often done using a VectorStore and Embeddings model.\\n\\n\\n\\u200bLoading documents\\nWe need to first load the blog post contents. We can use DocumentLoaders for this, which are objects that load in data from a source and return a list of Document objects.\\nIn this case weâ€™ll use the WebBaseLoader, which uses urllib to load HTML from web URLs and BeautifulSoup to parse it to text. We can customize the HTML -> text parsing by passing in parameters into the BeautifulSoup parser via bs_kwargs (see BeautifulSoup docs). In this case only HTML tags with class â€œpost-contentâ€, â€œpost-titleâ€, or â€œpost-headerâ€ are relevant, so weâ€™ll remove all others.\\nCopyimport bs4\\nfrom langchain_community.document_loaders import WebBaseLoader\\n\\n# Only keep post title, headers, and content from the full HTML.\\nbs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\\nloader = WebBaseLoader(\\n    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\\n    bs_kwargs={\"parse_only\": bs4_strainer},\\n)\\ndocs = loader.load()\\n\\nassert len(docs) == 1\\nprint(f\"Total characters: {len(docs[0].page_content)}\")\\n\\nCopyTotal characters: 43131\\n\\nCopyprint(docs[0].page_content[:500])\\n\\nCopy      LLM Powered Autonomous Agents\\n\\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn\\n\\nGo deeper\\nDocumentLoader: Object that loads data from a source as list of Documents.\\n\\nIntegrations: 160+ integrations to choose from.\\nBaseLoader: API reference for the base interface.\\n\\n\\u200bSplitting documents\\nOur loaded document is over 42k characters which is too long to fit into the context window of many models. Even for those models that could fit the full post in their context window, models can struggle to find information in very long inputs.\\nTo handle this weâ€™ll split the Document into chunks for embedding and vector storage. This should help us retrieve only the most relevant parts of the blog post at run time.\\nAs in the semantic search tutorial, we use a RecursiveCharacterTextSplitter, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases.\\nCopyfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\n\\ntext_splitter = RecursiveCharacterTextSplitter(\\n    chunk_size=1000,  # chunk size (characters)\\n    chunk_overlap=200,  # chunk overlap (characters)\\n    add_start_index=True,  # track index in original document\\n)\\nall_splits = text_splitter.split_documents(docs)\\n\\nprint(f\"Split blog post into {len(all_splits)} sub-documents.\")\\n\\nCopySplit blog post into 66 sub-documents.\\n\\nGo deeper\\nTextSplitter: Object that splits a list of Document objects into smaller\\nchunks for storage and retrieval.\\n\\nIntegrations\\nInterface: API reference for the base interface.\\n\\n\\u200bStoring documents\\nNow we need to index our 66 text chunks so that we can search over them at runtime. Following the semantic search tutorial, our approach is to embed the contents of each document split and insert these embeddings into a vector store. Given an input query, we can then use vector search to retrieve relevant documents.\\nWe can embed and store all of our document splits in a single command using the vector store and embeddings model selected at the start of the tutorial.\\nCopydocument_ids = vector_store.add_documents(documents=all_splits)\\n\\nprint(document_ids[:3])\\n\\nCopy[\\'07c18af6-ad58-479a-bfb1-d508033f9c64\\', \\'9000bf8e-1993-446f-8d4d-f4e507ba4b8f\\', \\'ba3b5d14-bed9-4f5f-88be-44c88aedc2e6\\']\\n\\nGo deeper\\nEmbeddings: Wrapper around a text embedding model, used for converting text to embeddings.\\n\\nIntegrations: 30+ integrations to choose from.\\nInterface: API reference for the base interface.\\n\\nVectorStore: Wrapper around a vector database, used for storing and querying embeddings.\\n\\nIntegrations: 40+ integrations to choose from.\\nInterface: API reference for the base interface.\\n\\nThis completes the Indexing portion of the pipeline. At this point we have a query-able vector store containing the chunked contents of our blog post. Given a user question, we should ideally be able to return the snippets of the blog post that answer the question.\\n\\u200b2. Retrieval and Generation\\nRAG applications commonly work as follows:\\n\\nRetrieve: Given a user input, relevant splits are retrieved from storage using a Retriever.\\nGenerate: A model produces an answer using a prompt that includes both the question with the retrieved data\\n\\n\\nNow letâ€™s write the actual application logic. We want to create a simple application that takes a user question, searches for documents relevant to that question, passes the retrieved documents and initial question to a model, and returns an answer.\\nWe will demonstrate:\\n\\nA RAG agent that executes searches with a simple tool. This is a good general-purpose implementation.\\nA two-step RAG chain that uses just a single LLM call per query. This is a fast and effective method for simple queries.\\n\\n\\u200bRAG agents\\nOne formulation of a RAG application is as a simple agent with a tool that retrieves information. We can assemble a minimal RAG agent by implementing a tool that wraps our vector store:\\nCopyfrom langchain.tools import tool\\n\\n@tool(response_format=\"content_and_artifact\")\\ndef retrieve_context(query: str):\\n    \"\"\"Retrieve information to help answer a query.\"\"\"\\n    retrieved_docs = vector_store.similarity_search(query, k=2)\\n    serialized = \"\\\\n\\\\n\".join(\\n        (f\"Source: {doc.metadata}\\\\nContent: {doc.page_content}\")\\n        for doc in retrieved_docs\\n    )\\n    return serialized, retrieved_docs\\n\\nHere we use the tool decorator to configure the tool to attach raw documents as artifacts to each ToolMessage. This will let us access document metadata in our application, separate from the stringified representation that is sent to the model.\\nRetrieval tools are not limited to a single string query argument, as in the above example. You can\\nforce the LLM to specify additional search parameters by adding argumentsâ€” for example, a category:Copyfrom typing import Literal\\n\\ndef retrieve_context(query: str, section: Literal[\"beginning\", \"middle\", \"end\"]):\\n\\nGiven our tool, we can construct the agent:\\nCopyfrom langchain.agents import create_agent\\n\\n\\ntools = [retrieve_context]\\n# If desired, specify custom instructions\\nprompt = (\\n    \"You have access to a tool that retrieves context from a blog post. \"\\n    \"Use the tool to help answer user queries.\"\\n)\\nagent = create_agent(model, tools, system_prompt=prompt)\\n\\nLetâ€™s test this out. We construct a question that would typically require an iterative sequence of retrieval steps to answer:\\nCopyquery = (\\n    \"What is the standard method for Task Decomposition?\\\\n\\\\n\"\\n    \"Once you get the answer, look up common extensions of that method.\"\\n)\\n\\nfor event in agent.stream(\\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\\n    stream_mode=\"values\",\\n):\\n    event[\"messages\"][-1].pretty_print()\\n\\nCopy================================ Human Message =================================\\n\\nWhat is the standard method for Task Decomposition?\\n\\nOnce you get the answer, look up common extensions of that method.\\n================================== Ai Message ==================================\\nTool Calls:\\n  retrieve_context (call_d6AVxICMPQYwAKj9lgH4E337)\\n Call ID: call_d6AVxICMPQYwAKj9lgH4E337\\n  Args:\\n    query: standard method for Task Decomposition\\n================================= Tool Message =================================\\nName: retrieve_context\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Task decomposition can be done...\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Component One: Planning...\\n================================== Ai Message ==================================\\nTool Calls:\\n  retrieve_context (call_0dbMOw7266jvETbXWn4JqWpR)\\n Call ID: call_0dbMOw7266jvETbXWn4JqWpR\\n  Args:\\n    query: common extensions of the standard method for Task Decomposition\\n================================= Tool Message =================================\\nName: retrieve_context\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Task decomposition can be done...\\n\\nSource: {\\'source\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\'}\\nContent: Component One: Planning...\\n================================== Ai Message ==================================\\n\\nThe standard method for Task Decomposition often used is the Chain of Thought (CoT)...\\n\\nNote that the agent:\\n\\nGenerates a query to search for a standard method for task decomposition;\\nReceiving the answer, generates a second query to search for common extensions of it;\\nHaving received all necessary context, answers the question.\\n\\nWe can see the full sequence of steps, along with latency and other metadata, in the LangSmith trace.\\nYou can add a deeper level of control and customization using the LangGraph framework directlyâ€” for example, you can add steps to grade document relevance and rewrite search queries. Check out LangGraphâ€™s Agentic RAG tutorial for more advanced formulations.\\n\\u200bRAG chains\\nIn the above agentic RAG formulation we allow the LLM to use its discretion in generating a tool call to help answer user queries. This is a good general-purpose solution, but comes with some trade-offs:\\nâœ… Benefitsâš ï¸ DrawbacksSearch only when needed â€“ The LLM can handle greetings, follow-ups, and simple queries without triggering unnecessary searches.Two inference calls â€“ When a search is performed, it requires one call to generate the query and another to produce the final response.Contextual search queries â€“ By treating search as a tool with a query input, the LLM crafts its own queries that incorporate conversational context.Reduced control â€“ The LLM may skip searches when they are actually needed, or issue extra searches when unnecessary.Multiple searches allowed â€“ The LLM can execute several searches in support of a single user query.\\nAnother common approach is a two-step chain, in which we always run a search (potentially using the raw user query) and incorporate the result as context for a single LLM query. This results in a single inference call per query, buying reduced latency at the expense of flexibility.\\nIn this approach we no longer call the model in a loop, but instead make a single pass.\\nWe can implement this chain by removing tools from the agent and instead incorporating the retrieval step into a custom prompt:\\nCopyfrom langchain.agents.middleware import dynamic_prompt, ModelRequest\\n\\n@dynamic_prompt\\ndef prompt_with_context(request: ModelRequest) -> str:\\n    \"\"\"Inject context into state messages.\"\"\"\\n    last_query = request.state[\"messages\"][-1].text\\n    retrieved_docs = vector_store.similarity_search(last_query)\\n\\n    docs_content = \"\\\\n\\\\n\".join(doc.page_content for doc in retrieved_docs)\\n\\n    system_message = (\\n        \"You are a helpful assistant. Use the following context in your response:\"\\n        f\"\\\\n\\\\n{docs_content}\"\\n    )\\n\\n    return system_message\\n\\n\\nagent = create_agent(model, tools=[], middleware=[prompt_with_context])\\n\\nLetâ€™s try this out:\\nCopyquery = \"What is task decomposition?\"\\nfor step in agent.stream(\\n    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\\n    stream_mode=\"values\",\\n):\\n    step[\"messages\"][-1].pretty_print()\\n\\nCopy================================ Human Message =================================\\n\\nWhat is task decomposition?\\n================================== Ai Message ==================================\\n\\nTask decomposition is...\\n\\nIn the LangSmith trace we can see the retrieved context incorporated into the model prompt.\\nThis is a fast and effective method for simple queries in constrained settings, when we typically do want to run user queries through semantic search to pull additional context.\\nReturning source documentsThe above RAG chain incorporates retrieved context into a single system message for that run.As in the agentic RAG formulation, we sometimes want to include raw source documents in the application state to have access to document metadata. We can do this for the two-step chain case by:\\nAdding a key to the state to store the retrieved documents\\nAdding a new node via a pre-model hook to populate that key (as well as inject the context).\\nCopyfrom typing import Any\\nfrom langchain_core.documents import Document\\nfrom langchain.agents.middleware import AgentMiddleware, AgentState\\n\\n\\nclass State(AgentState):\\n    context: list[Document]\\n\\n\\nclass RetrieveDocumentsMiddleware(AgentMiddleware[State]):\\n    state_schema = State\\n\\n    def before_model(self, state: AgentState) -> dict[str, Any] | None:\\n        last_message = state[\"messages\"][-1]\\n        retrieved_docs = vector_store.similarity_search(last_message.text)\\n\\n        docs_content = \"\\\\n\\\\n\".join(doc.page_content for doc in retrieved_docs)\\n\\n        augmented_message_content = (\\n            f\"{last_message.text}\\\\n\\\\n\"\\n            \"Use the following context to answer the query:\\\\n\"\\n            f\"{docs_content}\"\\n        )\\n        return {\\n            \"messages\": [last_message.model_copy(update={\"content\": augmented_message_content})],\\n            \"context\": retrieved_docs,\\n        }\\n\\n\\nagent = create_agent(\\n    model,\\n    tools=[],\\n    middleware=[RetrieveDocumentsMiddleware()],\\n)\\n\\n\\u200bNext steps\\nNow that weâ€™ve implemented a simple RAG application via create_agent, we can easily incorporate new features and go deeper:\\n\\nStream tokens and other information for responsive user experiences\\nAdd conversational memory to support multi-turn interactions\\nAdd long-term memory to support memory across conversational threads\\nAdd structured responses\\nDeploy your application with LangSmith Deployment\\n\\n\\nEdit this page on GitHub or file an issue.\\nConnect these docs to Claude, VSCode, and more via MCP for real-time answers.Was this page helpful?YesNoBuild a semantic search engine with LangChainPreviousBuild a SQL agentNextâŒ˜IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by')]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langchain_urls=[\n",
    "    \"https://python.langchain.com/docs/tutorials/\",\n",
    "    \"https://python.langchain.com/docs/tutorials/chatbot/\",\n",
    "    \"https://python.langchain.com/docs/tutorials/qa_chat_history/\"\n",
    "]\n",
    "\n",
    "docs=[WebBaseLoader(url).load() for url in langchain_urls]\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f834d0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=100\n",
    ")\n",
    "\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "## Add alll these text to vectordb\n",
    "\n",
    "vectorstorelangchain=FAISS.from_documents(\n",
    "    documents=doc_splits,\n",
    "    embedding=OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "\n",
    "retrieverlangchain=vectorstorelangchain.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf70c2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import create_retriever_tool\n",
    "\n",
    "retriever_tool_langchain=create_retriever_tool(\n",
    "    retrieverlangchain,\n",
    "    \"retriever_vector_langchain_blog\",\n",
    "    \"Search and run information about Langchain\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "edbc7887",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools=[retriever_tool,retriever_tool_langchain]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760a65ca",
   "metadata": {},
   "source": [
    "### LangGraph Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "127c309b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Sequence\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    # The add_messages function defines how an update should be processed\n",
    "    # Default is to replace. add_messages says \"append\"\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b581a0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 8, 'total_tokens': 17, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_c4585b5b9c', 'id': 'chatcmpl-CuevrLJBg02NPa0kBx1kadncg1R8O', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b8e58-da86-7b21-a00d-fd8c2b27123c-0', usage_metadata={'input_tokens': 8, 'output_tokens': 9, 'total_tokens': 17, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "llm.invoke(\"Hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f5176f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent(state):\n",
    "    \"\"\"\n",
    "    Invokes the agent model to generate a response based on the current state. Given\n",
    "    the question, it will decide to retrieve using the retriever tool, or simply end.\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "        dict: The updated state with the agent response appended to messages\n",
    "    \"\"\"\n",
    "    print(\"---CALL AGENT---\")\n",
    "    messages = state[\"messages\"]\n",
    "    model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "    model = model.bind_tools(tools)\n",
    "    response = model.invoke(messages)\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "67749188",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Literal, Sequence\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ed80d86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Edges\n",
    "def grade_documents(state) -> Literal[\"generate\", \"rewrite\"]:\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "        str: A decision for whether the documents are relevant or not\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK RELEVANCE---\")\n",
    "\n",
    "    # Data model\n",
    "    class grade(BaseModel):\n",
    "        \"\"\"Binary score for relevance check.\"\"\"\n",
    "\n",
    "        binary_score: str = Field(description=\"Relevance score 'yes' or 'no'\")\n",
    "\n",
    "    # LLM\n",
    "    model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "    # LLM with tool and validation\n",
    "    llm_with_tool = model.with_structured_output(grade)\n",
    "\n",
    "    # Prompt\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "        Here is the retrieved document: \\n\\n {context} \\n\\n\n",
    "        Here is the user question: {question} \\n\n",
    "        If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "        Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\",\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "    )\n",
    "\n",
    "    # Chain\n",
    "    chain = prompt | llm_with_tool\n",
    "\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    question = messages[0].content\n",
    "    docs = last_message.content\n",
    "\n",
    "    scored_result = chain.invoke({\"question\": question, \"context\": docs})\n",
    "\n",
    "    score = scored_result.binary_score\n",
    "\n",
    "    if score == \"yes\":\n",
    "        print(\"---DECISION: DOCS RELEVANT---\")\n",
    "        return \"generate\"\n",
    "\n",
    "    else:\n",
    "        print(\"---DECISION: DOCS NOT RELEVANT---\")\n",
    "        print(score)\n",
    "        return \"rewrite\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "552f7f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchainhub.client import Client\n",
    "\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "884aa9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "         dict: The updated message\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0].content\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    docs = last_message.content\n",
    "\n",
    "    # Prompt\n",
    "    prompt = client.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "    # LLM\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "    # Post-processing\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    # Chain\n",
    "    rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    # Run\n",
    "    response = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aa2a563b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite(state):\n",
    "    \"\"\"\n",
    "    Transform the query to produce a better question.\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "        dict: The updated state with re-phrased question\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0].content\n",
    "\n",
    "    msg = [\n",
    "        HumanMessage(\n",
    "            content=f\"\"\" \\n \n",
    "    Look at the input and try to reason about the underlying semantic intent / meaning. \\n \n",
    "    Here is the initial question:\n",
    "    \\n ------- \\n\n",
    "    {question} \n",
    "    \\n ------- \\n\n",
    "    Formulate an improved question: \"\"\",\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Grader\n",
    "    model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "    response = model.invoke(msg)\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "26bbd7fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.prebuilt import tools_condition\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Define the nodes we will cycle between\n",
    "workflow.add_node(\"agent\", agent)  # agent\n",
    "retrieve = ToolNode([retriever_tool,retriever_tool_langchain])\n",
    "workflow.add_node(\"retrieve\", retrieve)  # retrieval\n",
    "workflow.add_node(\"rewrite\", rewrite)  # Re-writing the question\n",
    "workflow.add_node(\n",
    "    \"generate\", generate\n",
    ")  # Generating a response after we know the documents are relevant\n",
    "# Call agent node to decide to retrieve or not\n",
    "workflow.add_edge(START, \"agent\")\n",
    "\n",
    "# Decide whether to retrieve\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    # Assess agent decision\n",
    "    tools_condition,\n",
    "    {\n",
    "        # Translate the condition outputs to nodes in our graph\n",
    "        \"tools\": \"retrieve\",\n",
    "        \"stop\": END,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Edges taken after the `action` node is called.\n",
    "workflow.add_conditional_edges(\n",
    "    \"retrieve\",\n",
    "    # Assess agent decision\n",
    "    grade_documents,\n",
    ")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "workflow.add_edge(\"rewrite\", \"agent\")\n",
    "\n",
    "# Compile\n",
    "graph = workflow.compile()\n",
    "from IPython.display import Image, display\n",
    "display(Image(graph.get_graph(xray=True).draw_mermaid_png()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "661decb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CALL AGENT---\n",
      "---CHECK RELEVANCE---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nayanverma/learning/RAG/.venv/lib/python3.14/site-packages/langchain_core/runnables/utils.py:750: DeprecationWarning: 'asyncio.iscoroutinefunction' is deprecated and slated for removal in Python 3.16; use inspect.iscoroutinefunction() instead\n",
      "  return asyncio.iscoroutinefunction(func) or (\n",
      "/Users/nayanverma/learning/RAG/.venv/lib/python3.14/site-packages/langchain_core/runnables/utils.py:752: DeprecationWarning: 'asyncio.iscoroutinefunction' is deprecated and slated for removal in Python 3.16; use inspect.iscoroutinefunction() instead\n",
      "  and asyncio.iscoroutinefunction(func.__call__)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---DECISION: DOCS NOT RELEVANT---\n",
      "no\n",
      "---TRANSFORM QUERY---\n",
      "---CALL AGENT---\n",
      "---CHECK RELEVANCE---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nayanverma/learning/RAG/.venv/lib/python3.14/site-packages/langchain_core/runnables/utils.py:750: DeprecationWarning: 'asyncio.iscoroutinefunction' is deprecated and slated for removal in Python 3.16; use inspect.iscoroutinefunction() instead\n",
      "  return asyncio.iscoroutinefunction(func) or (\n",
      "/Users/nayanverma/learning/RAG/.venv/lib/python3.14/site-packages/langchain_core/runnables/utils.py:752: DeprecationWarning: 'asyncio.iscoroutinefunction' is deprecated and slated for removal in Python 3.16; use inspect.iscoroutinefunction() instead\n",
      "  and asyncio.iscoroutinefunction(func.__call__)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---DECISION: DOCS NOT RELEVANT---\n",
      "no\n",
      "---TRANSFORM QUERY---\n",
      "---CALL AGENT---\n",
      "---CHECK RELEVANCE---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nayanverma/learning/RAG/.venv/lib/python3.14/site-packages/langchain_core/runnables/utils.py:750: DeprecationWarning: 'asyncio.iscoroutinefunction' is deprecated and slated for removal in Python 3.16; use inspect.iscoroutinefunction() instead\n",
      "  return asyncio.iscoroutinefunction(func) or (\n",
      "/Users/nayanverma/learning/RAG/.venv/lib/python3.14/site-packages/langchain_core/runnables/utils.py:752: DeprecationWarning: 'asyncio.iscoroutinefunction' is deprecated and slated for removal in Python 3.16; use inspect.iscoroutinefunction() instead\n",
      "  and asyncio.iscoroutinefunction(func.__call__)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---DECISION: DOCS NOT RELEVANT---\n",
      "no\n",
      "---TRANSFORM QUERY---\n",
      "---CALL AGENT---\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmessages\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HumanMessage\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mgraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mHumanMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWhat is LangGraph?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/learning/RAG/.venv/lib/python3.14/site-packages/langgraph/pregel/main.py:3068\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[39m\n\u001b[32m   3065\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | Any] = []\n\u001b[32m   3066\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m3068\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3069\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3070\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3071\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3072\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mupdates\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   3073\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   3074\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3075\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3076\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3077\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3078\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3079\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3080\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3081\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3082\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   3083\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/learning/RAG/.venv/lib/python3.14/site-packages/langgraph/pregel/main.py:2643\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2641\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2642\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2643\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2644\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2645\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2646\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2647\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2648\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2650\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmpty\u001b[49m\n\u001b[32m   2652\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2653\u001b[39m loop.after_tick()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/learning/RAG/.venv/lib/python3.14/site-packages/langgraph/pregel/_runner.py:167\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    165\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m                \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/learning/RAG/.venv/lib/python3.14/site-packages/langgraph/pregel/_retry.py:42\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     40\u001b[39m     task.writes.clear()\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     44\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/learning/RAG/.venv/lib/python3.14/site-packages/langgraph/_internal/_runnable.py:656\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    654\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    655\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m656\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    658\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/learning/RAG/.venv/lib/python3.14/site-packages/langgraph/_internal/_runnable.py:400\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    398\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36magent\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m     14\u001b[39m model = ChatOpenAI(model=\u001b[33m\"\u001b[39m\u001b[33mgpt-4o-mini\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     15\u001b[39m model = model.bind_tools(tools)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m response = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# We return a list, because this will get added to the existing list\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: [response]}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/learning/RAG/.venv/lib/python3.14/site-packages/langchain_core/runnables/base.py:5557\u001b[39m, in \u001b[36mRunnableBindingBase.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   5550\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   5551\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m   5552\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5555\u001b[39m     **kwargs: Any | \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   5556\u001b[39m ) -> Output:\n\u001b[32m-> \u001b[39m\u001b[32m5557\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbound\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5558\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   5559\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5560\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m{\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5561\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/learning/RAG/.venv/lib/python3.14/site-packages/langchain_core/language_models/chat_models.py:398\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    384\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    385\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    386\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    391\u001b[39m     **kwargs: Any,\n\u001b[32m    392\u001b[39m ) -> AIMessage:\n\u001b[32m    393\u001b[39m     config = ensure_config(config)\n\u001b[32m    394\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    395\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAIMessage\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    396\u001b[39m         cast(\n\u001b[32m    397\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    408\u001b[39m         ).message,\n\u001b[32m    409\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/learning/RAG/.venv/lib/python3.14/site-packages/langchain_core/language_models/chat_models.py:1117\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1108\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m   1110\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1114\u001b[39m     **kwargs: Any,\n\u001b[32m   1115\u001b[39m ) -> LLMResult:\n\u001b[32m   1116\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/learning/RAG/.venv/lib/python3.14/site-packages/langchain_core/language_models/chat_models.py:927\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    924\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    925\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    926\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m927\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    933\u001b[39m         )\n\u001b[32m    934\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    935\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/learning/RAG/.venv/lib/python3.14/site-packages/langchain_core/language_models/chat_models.py:1221\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1219\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1220\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1221\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1222\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1223\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1224\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1225\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/learning/RAG/.venv/lib/python3.14/site-packages/langchain_openai/chat_models/base.py:1375\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1368\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _construct_lc_result_from_responses_api(\n\u001b[32m   1369\u001b[39m             response,\n\u001b[32m   1370\u001b[39m             schema=original_schema_obj,\n\u001b[32m   1371\u001b[39m             metadata=generation_info,\n\u001b[32m   1372\u001b[39m             output_version=\u001b[38;5;28mself\u001b[39m.output_version,\n\u001b[32m   1373\u001b[39m         )\n\u001b[32m   1374\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1375\u001b[39m         raw_response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwith_raw_response\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1376\u001b[39m         response = raw_response.parse()\n\u001b[32m   1377\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/learning/RAG/.venv/lib/python3.14/site-packages/openai/_legacy_response.py:364\u001b[39m, in \u001b[36mto_raw_response_wrapper.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    360\u001b[39m extra_headers[RAW_RESPONSE_HEADER] = \u001b[33m\"\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    362\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33mextra_headers\u001b[39m\u001b[33m\"\u001b[39m] = extra_headers\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(LegacyAPIResponse[R], \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/learning/RAG/.venv/lib/python3.14/site-packages/openai/_utils/_utils.py:286\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    284\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/learning/RAG/.venv/lib/python3.14/site-packages/openai/resources/chat/completions/completions.py:1192\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1145\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1146\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1147\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1189\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m   1190\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m   1191\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m1192\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1197\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1198\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1199\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1200\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1201\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1202\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1203\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1204\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1205\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1206\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1207\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1208\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1209\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1210\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1211\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1212\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1213\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_retention\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_retention\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1214\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1215\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1216\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafety_identifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1217\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1218\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1219\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1220\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1221\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1222\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1223\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1224\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1225\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1226\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1227\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1228\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1229\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mverbosity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1230\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1231\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1232\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m   1233\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m   1234\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1235\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1236\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1237\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m   1238\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1239\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1240\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1241\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1242\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/learning/RAG/.venv/lib/python3.14/site-packages/openai/_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/learning/RAG/.venv/lib/python3.14/site-packages/openai/_base_client.py:982\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m    980\u001b[39m response = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    981\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m982\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    984\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    985\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    986\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    987\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    988\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mEncountered httpx.TimeoutException\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/learning/RAG/.venv/lib/python3.14/site-packages/httpx/_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/learning/RAG/.venv/lib/python3.14/site-packages/httpx/_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/learning/RAG/.venv/lib/python3.14/site-packages/httpx/_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/learning/RAG/.venv/lib/python3.14/site-packages/httpx/_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1011\u001b[39m     )\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1018\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/learning/RAG/.venv/lib/python3.14/site-packages/httpx/_transports/default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    255\u001b[39m     status_code=resp.status,\n\u001b[32m    256\u001b[39m     headers=resp.headers,\n\u001b[32m    257\u001b[39m     stream=ResponseStream(resp.stream),\n\u001b[32m    258\u001b[39m     extensions=resp.extensions,\n\u001b[32m    259\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/learning/RAG/.venv/lib/python3.14/site-packages/httpcore/_sync/connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/learning/RAG/.venv/lib/python3.14/site-packages/httpcore/_sync/connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/learning/RAG/.venv/lib/python3.14/site-packages/httpcore/_sync/connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/learning/RAG/.venv/lib/python3.14/site-packages/httpcore/_sync/http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/learning/RAG/.venv/lib/python3.14/site-packages/httpcore/_sync/http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/learning/RAG/.venv/lib/python3.14/site-packages/httpcore/_sync/http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/learning/RAG/.venv/lib/python3.14/site-packages/httpcore/_sync/http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/learning/RAG/.venv/lib/python3.14/site-packages/httpcore/_backends/sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/ssl.py:1285\u001b[39m, in \u001b[36mSSLSocket.recv\u001b[39m\u001b[34m(self, buflen, flags)\u001b[39m\n\u001b[32m   1281\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1282\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1283\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1284\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1286\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1287\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv(buflen, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/ssl.py:1140\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1138\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[32m   1139\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1140\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1141\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[32m   1142\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m x.args[\u001b[32m0\u001b[39m] == SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.suppress_ragged_eofs:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "graph.invoke({\n",
    "    \"messages\": [HumanMessage(content=\"What is LangGraph?\")]\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "448a376d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CALL AGENT---\n",
      "---CHECK RELEVANCE---\n",
      "---DECISION: DOCS RELEVANT---\n",
      "---GENERATE---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='What is Langchain?', additional_kwargs={}, response_metadata={}, id='b59cf3d6-8a02-4fd0-b641-738c811a4d66'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_sex2', 'function': {'arguments': '{\"query\": \"What is Langchain?\"}', 'name': 'retriever_vector_langchain_blog'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 186, 'prompt_tokens': 244, 'total_tokens': 430, 'completion_time': 0.461619046, 'prompt_time': 0.032314096, 'queue_time': 0.500323847, 'total_time': 0.493933142}, 'model_name': 'qwen-qwq-32b', 'system_fingerprint': 'fp_3796682456', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-7ac5a7b9-74fd-4c79-bc87-609019def050-0', tool_calls=[{'name': 'retriever_vector_langchain_blog', 'args': {'query': 'What is Langchain?'}, 'id': 'call_sex2', 'type': 'tool_call'}], usage_metadata={'input_tokens': 244, 'output_tokens': 186, 'total_tokens': 430}),\n",
       "  ToolMessage(content=\"Tutorials | ðŸ¦œï¸ðŸ”— LangChain\\n\\nNew to LangChain or LLM app development in general? Read this material to quickly get up and running building your first applications.\\nGet started\\u200b\\nFamiliarize yourself with LangChain's open-source components by building simple applications.\\nIf you're looking to get started with chat models, vector stores,\\nor other LangChain components from a specific provider, check out our supported integrations.\\n\\nSkip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1ðŸ’¬SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval\\n\\nSkip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1ðŸ’¬SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval\", name='retriever_vector_langchain_blog', id='e2edbaaa-b454-479b-a1f6-c2a579e732e9', tool_call_id='call_sex2'),\n",
       "  HumanMessage(content=\"\\n<think>\\nOkay, I need to figure out what LangChain is based on the provided context. Let me read through the context again. The page mentions tutorials for building applications with LangChain, chat models, vector stores, and RAG apps. There's also talk about integrations and components like LangSmith, LangGraph, and LangChain Hub. The main focus seems to be on developing LLM applications, using tools like chatbots, retrieval augmented generation, and integration with various data sources. So, putting this together, LangChain is a framework or library that helps developers create applications leveraging large language models and different components for things like data retrieval and processing. It provides tools and structures to build things like Q&A systems, chatbots, and more by integrating with vector databases and other services. The answer should be concise, so I'll highlight the key points: it's an open-source framework for developing LLM apps, offers components for chat models, retrieval systems, and has tools like RAG and integrations.\\n</think>\\n\\nLangChain is an open-source framework designed to simplify the development of applications leveraging large language models (LLMs). It provides components like chat models, retrieval systems (e.g., RAG, vector stores), and tools for building chatbots, Q&A systems, and more. The ecosystem includes integrations with various data sources and tools like LangSmith for deployment.\", additional_kwargs={}, response_metadata={}, id='19185b99-a9f9-479c-b894-8196fae2c280')]}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.invoke({\"messages\":\"What is Langchain?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c03cb88a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CALL AGENT---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='What is Machine learning?', additional_kwargs={}, response_metadata={}, id='4e92fdbc-f8e4-4411-927c-837a4cea5b6b'),\n",
       "  AIMessage(content='Machine learning is a subset of artificial intelligence (AI) that focuses on developing algorithms and statistical models. These allow systems to improve at tasks through experience, typically by learning patterns from data, rather than relying solely on explicit programming.\\n\\n### Key Aspects:\\n1. **Data-Driven**: Systems learn from historical data to make predictions or decisions.\\n2. **Types of Learning**:\\n   - **Supervised Learning**: Uses labeled data (e.g., predicting house prices based on features like size or location).\\n   - **Unsupervised Learning**: Finds hidden patterns in unlabeled data (e.g., customer segmentation).\\n   - **Reinforcement Learning**: Learns by trial-and-error via rewards/penalties (e.g., game-playing AI).\\n\\n### Common Applications:\\n- Image/speech recognition\\n- Recommendation systems (e.g., Netflix, Spotify)\\n- Fraud detection\\n- Self-driving cars\\n\\nItâ€™s widely used in industries like healthcare, finance, and marketing to automate complex tasks and uncover insights.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 596, 'prompt_tokens': 244, 'total_tokens': 840, 'completion_time': 1.4810983260000001, 'prompt_time': 0.060779581, 'queue_time': 0.384742413, 'total_time': 1.541877907}, 'model_name': 'qwen-qwq-32b', 'system_fingerprint': 'fp_3796682456', 'finish_reason': 'stop', 'logprobs': None}, id='run-a6ec3703-0c71-4ef9-bdb9-036b790858d6-0', usage_metadata={'input_tokens': 244, 'output_tokens': 596, 'total_tokens': 840})]}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.invoke({\"messages\":\"What is Machine learning?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2427a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
